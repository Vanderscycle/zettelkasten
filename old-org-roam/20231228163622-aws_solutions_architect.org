:PROPERTIES:
:ID:       24d43f89-27be-44a7-8a31-0a949dbf96b6
:ROAM_ALIASES: test
:END:
#+title: Certs - AWS Solutions Architect

* Usefull guide/helps
https://codingnconcepts.com/aws/aws-certified-solutions-architect-associate/#ec2-elastic-compute-cloud
* Knowledge
** Networking
*** VPC (Virtual Private Cloud)
Allow for resources to be isolated while in the same account giving full control over networking:
- Subnets: IP address
- Route tables: what path information to and from takes place
- Firewall: Security Groups/NACLS
- Gateways
*** NOTES: VPC are specific to a single region only

Every VPC has a range of IP addresses assigned through a CIDR block. Can only be between size 16 and 28
#+begin_example
0 -> 255 = 8 bits
192.168.0.0/16 grants
192.168.0.0 -> 192.168.255.255

0 -> 15 = 4 bits
192.168.0.0/28 grants
192.168.0.1 -> 192.168.0.14
#+end_example

AWS gives a default VPC with a default config, a custom VPC is configured by yourself.
Default:
- One Default VPC per region
- /16 ipv4 CIDR 172.31.0.0/16
- /20 default subnet in each AZ 
- Internet gateway
- a route that points all traffic (0.0.0.0/0) accessible from the internet
  
*** Subnets
Groups of IP address within a VPC.
- Subnets within a VPC must be within the CIDR range.
- subnet block size must be between /16 /28
- the first 4 IP addresses of a subnet are reserved and cannot be used e.g. =192.168.10.1 -> 192.168.10.3=
#+begin_example
10.0.1.0/24
10.0.1.0 -> 10.0.1.255
with the exception of the first 3 and the last one
#+end_example
**** CRIT: Subnets cannot overlap with other subnets in the vpc. Still be careful about VPC peering
- Subnets can communicate with other subnets in the VPC (by default)
There is an option to auto-assign public ipv4/ipv6 to a private address
Subnets can be made public/private using internet gateways/nat gateways

*** Route Table
Each VPC has a VPC router which has a router interface in each subnet of the vpc using the first address of each subnet for traffic routing. e.g. =10.0.1.0/24= will give =10.0.1.1= Its main purpose is to wrap traffic between subnets and VPC.
Route tables is a set of rules that the router uses to forward traffic. Each rules are =routes= using the destination ip to match the target resources/ip/gateway/ec2
AWS gives preference to overlaping packet that uses a greater rule e.g. /24 > /16
**** CRIT: One subnet can only be associated with one route table. A route table may have more than one subnet associated to

*** Internet Gateways
To make a subnet public you must use an Internet Gateway and, using a route table, points the default =0.0.0.0/0= to the IGW. IGW are region and VPC specific. 
**** CRIT: An internet gateway can only be attached to one VPC at a time. A VPC can only have one Internet Gateway

*** Nat Gateways
Used for Private subnets that needs access to the internet, but only if the connection is initiated from withing AWS and not from the internet. You still require a public subnet + IGW with =0.0.0.0/0= default route and the NAT Gateway deployed to the public subnet. You need to configure the private subnet to have a default route =0.0.0.0/0= that point to the nat gateway. Costs: Charged by the hour and per GB of data processed.
**** CRIT: NAT gateways aren't region resilient as it is only installed on a specific subnet. They are AZ-reliant service. Also uses Elactic IPs
If building for resilience/redundancy ensure that you deploy private sunets in different AZ, each with a NAT gateway. This means 1:1 relation private subnets:Route Tables each with a distinct NAT on a different public subnet

*** Private/Public subnets
Should devices on the internet be able to interact with our services? If its a webpage, then yes! If its a db, then no place it in a private subnet.

*** Lab
How to ssh into a private EC2/resource from a public EC2
#+begin_src bash

  scp -i ec2-user.pem ec2-user.pem ec2-user@<public-IP>:~/
  ssh -i ec2-user.pem ec2-user@<insert-public-ip>
  ssh -i ec2-user.pem ec2-user@<insert-private-ip>
  # for ubuntu images the default user is: ubuntu and not ec2-user
#+end_src

*** DNS(VPC)
auto-generated AWS dns for resources.
AWS DNS server can be accessed on the second IP of the VPC CIDR block as well as 169.254.169.253
Device private IPs will automatically be assinged a DNS entry
enabledDNSHostnames: whether the VPC supprot assigning public DNS hostnames to instances with public IP addresses (false by default)
enableDnsSupport: determines whether the VPC supports DNS resolution through AWS provided DNS server. If doing so you must provide your own dns resolver (either custom or 8.8.8.8)
#+begin_example
In a public EC2 you can check the resolv.conf
cat /etc/resolv.conf
If for example the vpc CIDR block is 10.0.0.0/16 the dns nameserver would be 10.0.0.2
#+end_example

Can be modified later

*** Elastic IP
AWS static IPv4 address reserved for your acct. By default upblics IPs are not static and, if an EC@ instances goes down, then it will get a new public IP. One elastic IP per resource can be assigned, more inquires charges
**** CRIT: region specific

*** Security Group and NACL (firewall)
Stateless firewall (Default firewalls): must be configured to allow both inbound & outbound traffic declaratively.
Statefull firewall: firewall that is smart enough to understand which request and response are part of the same connection (port-wise)

**** network access control list (NACL)
Filter traffic entering and leaving a subnet, are stateless firewalls so rules must be be for both inbound/outbound, and doesn't filter traffic within a subnet. Every subnet within a VPC must be assicuated with a NACL however a NACL can be associated with multiple subnets

**** Security Group
Act as statefull firewall for individual AWS resources. Unless SG rules are specified they are denied by default. More than one SG can be applied to a resources and will be merged at evaluation time.
nacl, the smaller the rule number the sooner they will be processed.

**** CRIT: NACL act as firewall for subnets while SG act as firewalls for individual AWS resources. NACL can allow or deny traffic while SG only allow.
**** CRIT: SG by default contains an inbound/outbound rule that allows for all traffic

*** Load Balancers
Acts as a single ingress point for multiple resources directing traffic.
3 types of load balancer:
- classic lb
- application lb (application L7): https until the ALB then http to the resources
- network lb (TCP/UDP L4): meant for apps that don't use http/https, faster than ALB, forwards the connections

You configure the availability zone to the LB by assigning subnets by deploying a Load balancing node to the specific subnet (public).
cross-zone LB: Allows for traffic to be rerouted between AZ compute resources

Public LB (deployed on public subnets) Private LB (deployed on private subnets). Uses cases: multiple API compute behind a public LB talking to a private LB (db connections)

Listener: forwards incoming request to a target group based on defined parameters e.g. app1.com vs app2.com/auth 
Target groups: forwards to specific compute resources
You can also configure health checks

*** Virtual Private Network (VPN)
Use case: communicate between on-premise network with AWS Virtual Private cloud (VPC) safely and securely. First you create a VPN gateway on the VPC and Customer gateway on-premise They both get an IP address and communicates between encrypted.
To route traffic between we can use a route in a routing table (static) or exchange route dynamically using bgp. (dynamic)
Costs: per hour per VPN connections + data transfer
limits 1.25 gbps bandwith

*** Direct Connect
Alternative to VPN, allows you to to =physically= connect with AWS. Essentially directly connect to a regional data center where AWS rents a few server racks.
Charged by port/hours + outbound data transfer. Faster + more secure than VPN

*** VPC Peering
reminder, a vpc acts as a network boundary unless a connection between them are established e.g. VPC peering connections.
No costs for VPC peering connections, no costs for data transfer within a VPC, data transfer across AZ through a VPC peering incurs charges.

#+begin_example
VPC 1 10.1.0.0/16
VPC 2 10.2.0.0/16
Create the connection then update each VPC's route table to access each other's VPConnection then update each VPC's route table
rt-1 in vpc-1
(dest) 10.2.0.0/16 (target) vpc-2 through peering connections
rt-2 in vpc-2
(dest) 10.1.0.0/16 (target) vpc-1 through peering connections
#+end_example

*** Transit Gateway
#+begin_example
Say you have 2 VPC 1->3 where 1 is connected to 2 and 2 to 3 through a vpc peering
VPC 1 cannot talk to vpc 3 through vpc 2 (transitive vpc not supported) 
Now say you have n vpc then it can get insane to manage.
#+end_example
The solution is to use a transit gateway where all VPCs connects to.
**** CRIT: must specify one subnet from each AZ (us-east-1, us-west-2, etc.) to be used by the transit gateway to route traffic
Quite usefull also if you have an on-prem datacenter and say 4 VPC. Instead of having a VPN connection/direct connect between each we can use one transit gateway.
Transit gateways can connect to one another.

*** PrivateLink
providing access to public resources like s3 bucket to our internal resources. Use to connect public AWS services to to other VPCs.

*** Cloudfront
Basically a content delivery network (CDN) that uses AWS edge locations to minimize latency. Its a web service that speeds up distrubution of static(s3 bucket)/dynamic(amazon lightsail) content using AWS edge location.

**** Cloudfront Architecture
Origin is the source location for content that will be cached by cloudfront edge's locations. Cached content at an edge location remains for a set time: Time To Live (TTL) default 24hrs
You can cache invalidate the content to clear the cache at the edge's location which are performed at a =distribution=
#+begin_example
/* # entire distribution
/file.txt # just that file
/imgs/* # all object in that dir
#+end_example

While Cloudfront gives you a DNS + SSL cert you can use custom domain/SSL cert using AWS certificate Manager (ACM).
Use cases: static websites (server side logic), media files

#+begin_example
Say you have one img (car.jpg) in an s3 bucket at root / that you want to distribute using cloudfront.
the cloudfront distribution address will require (dnsName.cloudfront.com/car.jpg)
#+end_example

*** Lambda@Edge
Allows to write lightweight function at edge location to manipulate requests and responses that flow through cloudfront.
| Trigger          | Description                                                                                       | Cache Interaction          |
|------------------+---------------------------------------------------------------------------------------------------+----------------------------|
| Viewer Request   | Executes after CloudFront receives a request from the viewer, before checking the cache.          | Before cache check         |
| Origin Request   | Executes only when CloudFront forwards a request to the origin, when the object is not in cache.  | Before forwarding to origin |
| Origin Response  | Executes after CloudFront gets the response from the origin, before caching the object.           | Before caching the response |
| Viewer Response  | Executes before CloudFront forwards the response to the viewer, regardless of caching.            | Before response to viewer  |

If your function is sub millisecond you can use cloudfront functions if longer (5-30sec) a lambda@edge function can work

*** Global Accelerator
an edge node that uses AWS backbone connection to optimize time take to reach apps, unlike cloudfront which is used for caching data.

*** route53
AWS managed DNS
- Can purchase a domain name through the registrar
Hosted zone is allocated to 4 nameservers by AWS

| Type  | Description                                                                                   | Purpose                                                                                                   |
|-------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------|
| A     | Maps a domain to the IP address (IPv4) of the computer hosting the domain.                     | Used for converting human-readable domain names into computer-readable IP addresses.                     |
| AAAA  | Maps a domain to the IP address (IPv6) of the computer hosting the domain.                     | Similar to A records, but for IPv6 addresses, which allows for a larger number of IP addresses.          |
| CNAME | Maps a domain to another domain name (canonical name) instead of an IP address.                | Used for aliasing one domain name to another domain name, useful for managing multiple services under one domain. |

*** Route 53 Application Recovery Controller

Automating backup location and recovery failures using a Application Recovery Controller (ARC). This continuously performs readiness checks.
new concept cell: all resourceds required for an application to operate independently
recovery group is a collection of cells that represent an application that I want to check for failover readiness e.g. (active + standby cell)


** Service Storage

*** Elastic Block Storage
**** CRIT: Block storage can both be mounted as a file storage and boot of it.

Separate from a EC2 instance, can be moved to another one and even attached to multiple ec2 (granted the write is controlled). They are AZ resilient e.g. us-east-1a.
Both the EBS and EC2 must be in the same availability zone. If data must be migrated from one AZ to the other AZ a snapshot (volume snapshot) must be made and EBS booted from in the correct AZ.

**** NOTES: IOPS: Input/output operations per second is an input/output performance measurement used to characterize computer storage devices

EBS Volume types:
- General purpose SSD gp2/3: balance price/performance recommended for most workload. multi-attach not supported
  - GP2 performance scales with size
- Provisioned IOPS SSD: IOPS intensive, most performant. multi-attach supported
- Throughput-optimized HDD and cold HDD: cheaper than ssd but slow: boot/multi-attached not supported
  - throughput optimized HDD: big data, data warehouse
  - Cold SSD: lowest cost storage 
- magnetic volumes: performance not important

costs: fast IOPS more costs, snapshots per gb/months

*** TODO move later
lsblk
sudo file -s /dev/drive # shows if theres' file
sudo mkfs -f xfs /dev/drive create file system
mkdir -p /mnt create mounting point
sudo mount /dev/drive /mnt
df -k shows drives
sudo blkid shows UUID of drive
/etc/fcstab ( to permananently mount the drive sys)

*** Instance Store

Temporary block level storage physically located on the compute instance (ec2). Volume associated with the ec2. lost if the ec2 changes hosts. you can tell because the public ip address would have changed.

*** Elastic File System (EFS)

VPC specific mounted on mount target (use one with AZ resilient) on a subnet which will give an ip address.
2 types General Purpose performance mode (latency-sensitive applications) and elastic throughput mode (automatically scales) and burst

sudo mkdir /mnt
sudo dnf -y install amazon-efs-utils
sudo mount.efs efs:id /directory (id comes from the AWS console)

*** FSx for windows

EFS for windows:
- 1 AWS FSx for Windows File Server, easily integrate with MS active Dir
- 2 AWS FSx for Lustre, high performance data, integratest with S3, dataSync (only supports linux)
- 3 AWS FSx for NetAPP ONTAP high performance accessible on Linux, macos, windows
- 4 AWS FSx for OpenZFS

Lustre: Lustre is a type of parallel distributed file system, generally used for large-scale cluster computing.  

*** Simple Storage Services (S3)

**** General
Object storage service, cannot be mounted to an ec2 like EBS. Think of it like Dropbox/google drive. Major usecase: since storing data on a webserver drive is expensive and cumbersome, its cheaper to store the data in s3 and fetch to the webserver as needed.

S3 has a flat file structure, even if you can create folders within it. Its more for pathing for prefix. It 
Restrictions:
- unlimited number of objects stored
- max individual file size 5TB
- AWS account can support up to 100 buckets

**** Storage classes
3 key criterias:
- Data acess
- Resiliency
- costs

Types:
***** Default
Objects copied across 3 AZ, costs per GB/month, low latency, costs per egress gb

***** S3 std-IA (infrequent access) 
Objects copied across 3 AZ, costs per GB/month, cheaper than s3 default, has a retrival fee + egress costs

***** S3 One Zone IA
Objects only present in one AZ, cheaper, but not resilient

***** S3 Glacier Instant
Low costs option for rarely accessed data (but available in miliseconds), multi AZ + low latency, has retrival fee 

***** S3 Glacier Flexible
Objects not immediately accessible (cold start), multi-az, retrival fees varies based on desired retrival speeds

***** S3 Glacier Deep archive
Objects not immediately accessible, multi-az, retrival fee, longer retrival time

***** S3 Intelligent-tiering
automatically reduces storage costs by intelligently moving data

**** S3 versioning 

Globally enabled in a bucket, allows to have multi version of a file, much like source control. Each object is given a versionID.
Charged for all version of a file. so if v1 and v2 are each 10gb you will be charged for 20gb total.

Cannot be disabled only suspended. If you add a new version of a file after versioning has been suspended then a null version-ID will be given and each new upload of the same file will replaced the null version.

You can have MFA delete

**** S3 ACL/Bucket policies
By default only the creater + root user can access the content of the bucket.
#+begin_src json
{
    "Version": "2012-10-17",
    "Id": "Policy1474996930912",
    "Statement": [ // can have more than one one statment
        {
            "Sid": "Stmt1474996926670",
            "Effect": "Allow", // either allow or deny
            "Principal": "*", // who it applies to in this case its all
            "Action": "s3:GetObject", // list of thing that the principal can/can't do.
            "Resource": "arn:aws:s3:::example-bucket/*"
        }
    ]
}
#+end_src

Another one
#+begin_src  json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowSpecificIPAccessToSpecificFolder",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::example-bucket/media/photos/*",
            "Condition": {
                "IpAddress": {
                    "aws:SourceIp": "192.0.2.0/24"
                }
            }
        }
    ]
}

#+end_src

If you want to expose the bucket to the world you also need to uncheck =block all public access= even with permessive bucket policy.
IAM policy are attached to authenticated users, resources policies are attached to resources (ec2/iam/etc.) and can also apply to unauthenticated user.

ACL policies are legacy (READ/WRITE/FULL_CONTRL) etc... do not use

**** Static hosting hosting (s3)

Does not work with any SSR (server side rendering). There are costs associated with GET requests per 1000. If you use a DNS you need to name your bucket the same e.b. bestcar.com bucket must be named bestcar.

**** Pres-signed urls
Share a few files to an unauthenticated user without giving that person an AWS acct. however the pre-signed url thinks its you. Usefull for private bucket for file sharing.
Another use case is to upload files to the bucket directly instead of making a POST request to the backend server.

**** S3 Access points
For use when legal/admin/infra/dev needs access to different folder or have different scope in a certain bucket. This would make for a very complicated bucket policy. You can restrict VPC using access points. Every group/user can be given their own access point.

*** AWS Backup/disaster recovery
Planning and responding to disaster

Backups: essential part of disaster recovery by creating copies of data to restore in case of data loss
Disaster recovery: encompasses a broader strategy, including backup. Planning for system and application recovery
- s3: multi az
- ebs: snapshots
- AWS backup: AWS service

Components:
- Backup vault, Backup plan, recovery point.
To use that service we need to create first a backup plan, schedule, duration.

*** Elastic Disaster recovery
Fully managed disaster recovery service for physical, virtual and cloud-based servers (other than AWS). Can use AWS as a recovery site instead of investing in on-premises disaster recovery infra.
Uses a template

*** Storage Gateway
Bridge between on-premise and cloud base storage. Can be used as an extension to on-premises, backups, disaster recovery (assists migrations). Either a virtual machine or physical server deployed on your on-prems.

There are 3 types: volume, file and tape. based on what you use on-prems
Volume stored:
- Data is stored locally on-prems, replicated on AWS s3, doesn't increase datacenter storage capacity (usefull backup/disaster recovery)
Volume cached:
- acts as a datacenter extension increases customers storage capacity.

File:
- stored in s3

Tape:
- Essentially for deep storage

*** Questions
A developer wants to trigger a Lambda function for processing images each time a new image is uploaded to their S3 bucket. Use S3 event notifications

A company wants to protect their S3 objects from being accidentally deleted or overwritten. Which S3 feature should they enable. A. S3 versioning + s3 object lock

An organization needs a solution to minimize downtime and data loss with fast, reliable recovery of physical, virtual, and cloud-based servers into AWS. Which service should they use?. A: AWS Elastic Disaster Recovery

A data engineer needs to upload large files over a high-latency network to S3. Which method should they use to maximize the upload efficiency? A: s3 multipart upload

A company wants to analyze their S3 object access patterns to determine when to transition objects to less expensive storage classes. Which tool can they use to obtain these insights? A. S3 analytics - storage class analysis

A database administrator is deploying a high-performance OLTP database system on AWS. Which Amazon EBS volume type should they choose to ensure consistent I/O performance and low latency? A. Provisioned IOPS SSD (io2)

A media company needs a high-performance file system for their compute-intensive workloads, such as high-performance computing (HPC), machine learning, and media data processing workflows. Which AWS service should they use? A. FSx lustre

A web development company wants to improve the upload speed to their S3 bucket for global users. Which feature should they enable on their S3 bucket? A. S3 transfer accel

** Service Compute

*** EC2
A server instance in the cloud.

**** Instances types
- General purpose: General good for most 
- Compute optimized:  more cpu power less of everything else (network/memory)
- Memory optimized: fast performance for workload processing large data in memory
- Storage optimized: best for read and write operations
- GPU instances: ML/AI

**** Amazon machine images (AMI)
What types of operating system the ec2 will use e.g. red hat, centos, etc.
There are also private AMIs (have your own OS iso images)
Also shared AMIs

**** ssh
to ssh into an ec2 you need an ssh key defined

**** Instances lifecyle
- pending state: init when an ec2 is first launched.
- running state: the default state
- stopping state: preparing to stop
- stopped state: ec2 is offline
- shutting down:
- terminated: instance is gone forever

Bootsrap script: you can pass a set of instructions for the ec2 to execute at start. e.g. install software/download repos. 
A security group must be defined. resource firewall
An EBS volume can be assigned to extend the storage of the ec2 (needs to be mounted) and snapshots can be taken. Gives persistent data storage
An ELB can allow for the traffice + autoscaling
An elastic IP can be assigned so that the EC2 doesn't change IP during reboots
**** Instances placements:
- cluster placement group: place ec2 as close as possible (low latency/high throughput) same server/rack
- Partition placement group: instances in one partitions do not share the same hardware as ones in other partitions + anciliaries
- spread placement group: each instances are placed on different hardware + anciliaries

**** EC2 Instances types 
- On demand: pay for compute capacity by the hour with no long term commitment.
- spot instances: bid on unused ec2 capacity. Code and application must be flexible enough to make use and can be shutdown at anytime.
- saving plans: low price on ec2 but requires a commitment 1 to 3 year term. (money)
- reserved instances: low price on ec2 but requires a commitment 1 to 3 year term. (compute power)
- dedicated hosts: renting your own dedicated ec2 hosts (but the server rack/host can change). great for licenses tied to machines
- dedicated instances: get your own server but unsure about which one.

*** EC2 Image builder
Much like dockerfiles AWS allows you to create images so that the ec2 instances generated from it starts will all of the required software/config/etc.

=Golder Image= the prefered organization image to base future images from
To create a golden/any image
1. select base image e.g. ubuntu
2. build phase (install all necessary tools/apps)
3. customize 
4. test
5. distribute

*** Elastic Network Interfaces
Virtual network card for your ec2. Essentially allows you to decouple the networking from the ec2 instance allowing for greater flexibility. Since it is separate from the ec2 you can attach it to any other ec2

There are primary and secondary ENIs:
- primary deleted with the ec2
- secondary and can be detached to another ec2 instance and with different SG. Can assign a specific elastic ip

*** Elastic Beanstalk
Allow to quickly deploy web applications without having to worry about the infrastructure like vpc, subnets, igw, ec2, eg, etc... just upload the code, configure beanstalk, and run.
environment: collection of all AWS resources for an environment to run.

Essentially, that services will create and manage all the relevant dbs, load balancers, sg. Could be usefull for prototyping.

*** Lightsail
The quickest way to deploy code, without having to understand everything about it.

*** Elastic Container Services (ECS)
AWS's version of kubernetes. Fully managed container orchestration service that helps manager, and scale containerized applications.
There are 2 types:
- ec2: you manually provision ec2, install docker and everything else. Servers are always running. Greater control + felxibility.
- Fargate (serverless): (AWS manages the underlying infra)

**** ECS task
Essentially the pod in kubernetes terms
**** ECS service
Essentially the deployment in kubernetes terms

*** Elastic Kubernetes Service
Managed kubernetes service by AWS (bring your own manifests files). Makes file easier for the following: (because running and scaling k8s can be difficult)
- control plane
- scaling and backups
- install control place processes

Also integrations with ssm, lbs, s3, rds etc.
3 options:
1. Self-managed nodes: create ec2 instances, install kubernets on them, update them (you handle all of them)
2. Managed node group: managed group of nodes
3. Fargate: serverless

*** Elastic container Repository (ECR)
store your docker containers iamages in AWS. Can be either public/private ecr

you can use AWS Codecommit to push your code (aws git repos) then AWS CodeBuild can be your CI/CD pipelines

*** App runner
AWS version of vercel. Can be used directly from a container (ECR), Can be tied with AWS CodeCommit, AWS CodeBuild and then AppRunner.
Can link with s3/dynamodb, etc.

*** AWS Batch
For jobs like compressings files, crunching data, etc that is in a sporadic nature instead of having an ec2 running half the time empty you can let AWS figure it out.
Allows to queue jobs, priorities,
Dynanmic resource provisioning, cost efficient

*** AWS Lambda
Serverless, event driven, compute service that can run code without provisioning servers. Set a trigger, lambda run, next-step

Example, trigger a lambda whenever an image file is uploaded to an s3 bucket and convert it from whatever format (.svg/jpeg) to png in an output bucklet

** Service Database

*** AWS Relational Databases (SQL)
Why use an RDS db over self hosting one/on prem?
- Low risk of hardware failure + downtine
- Disaster recovery
- security, access lockdown + compliance

Supported types: Postgresql, MariaDB, Oracle, MS SQL, MySQL
Instances types: General purposes vs memory optimized (best for workloads with large datasets)
Deployment types:
- Single RDS setup (single copy/single AZ). If the AZ goes down you lose your data. Its really cheap tho. Do not use for Prd. high latency 
- Multi-AZ RDS setup. Multiple copies
- RDS read replicas. You can have multiple read replicas but only one write replica. Allows the db to act as a load balancer. Can promote a read to a write in case of failure. Allows to scale out the read heavy database workloads.
- RDS Cross-region read replicas. Can set read replicas in Asia for example to reduce latancy
- Multi-AZ cluster

Blue/green deployment. Blue production/green staging. Allows to test changes to the db without affecting prod. Expensive because you are running a double rds cluster instances.
Storage types: GP SSD, IOPS SSD (low I/O and consistent throughput), magnetic (HDD) to be phased out
**** RDS Configurations:
DB parameter groups: customize db
DB Option Groups:
DB Subnet Groups:
DB SG: access 
DB snapshots: backups
performance insights

*** Aurora/Aurora serverless
Can act as drop-in replacement from mysql and postgresql. DB is typically a combination of compute and storage in a single instance. Aurora decouples compute and storage for easier scalability, durability, and availability.

Continuous backup to s3, storage volume spread accross multiple AZ. Storage volume segmented in 10gb protection nodes (PG), each PG contains 6 10gb segments and copies of the same data on different storage nodes (two in each AZ).

Due to its configuration it can expand up to 128TBs without downtime. Uses quorum model for write + read (4/6 writes must agree), maintains write capability if an AZ fails.

https://geekflare.com/wp-content/uploads/2022/12/Screenshot-2022-12-06-at-23.47.45-1500x795.png

aurora types: provisioned (fixed Capacity/global), serverless (on-demand scaling.usefull for variable and unpredictable workloads)
Unit of measure is Aurora capacity unit (ACU): 1 ACU = 2 gib mem corresponding cpu/networking
Aurora global: provision data across the globe. Uses the same structure as above but everything is cluster
https://digitalcloud.training/wp-content/uploads/2022/01/aurora-global-database.jpeg

*** Redshift (data warehouse)
Fully managed petabyte-scale data warehouse service in the cloud. Supports client connections with business intelligence (BI), reporting data and analytics tools.
https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2021/01/20/ARYIR-1.jpg

Designed to consolidate data from various location, optimized for analytics (unlike std dbs which are optimzed for transactions), mostly used for historical analysis. Data transformation focus (ETL). Cost efficient for data analytics

features:
- integrations (s3, etc.)
- columnar stoage
- Data compression
- scalability
- data ingestion
- sql compatibility
- integration wiht BI tools (tableau)
- data lake ingestion

Uses cases:
financial + demand forecasts
optimize BI

*** Redshift Serverless
Same as provisioned Redshift but serverless but in a hybrid method using (base capacity). Power calculated in Redshift Processing Units (RPUs) where 1 RPU = 16 GB RAM. You pay for the workloads you run in RPU/hrs on a per-second basis.

Start a Base capacity + extra using RPU (8-512RPU). 8-24 RPU can support up to 128 TB of data greater than that you need to use 32 RPU min.


*** DynamoDB (NoSQL)
Managed service
Components:
- Tables: collection of data
- Items: entry within a table
- Attributes: key/value of an item
 #+begin_src json
[
  {
    "UserId": "user1",
    "Email": "user1@example.com",
    "Name": "John Doe",
    "SignupDate": "2024-04-08",
    "Interests": ["Reading", "Hiking", "Coding"]
  },
  {
    "UserId": "user2",
    "Email": "user2@example.com",
    "LastLoginDate": "2024-04-09",
    "PurchaseHistory": [
      {
        "Item": "Laptop",
        "Date": "2024-03-15",
        "Amount": 1200
      },
      {
        "Item": "Headphones",
        "Date": "2024-03-20",
        "Amount": 150
      }
    ]
  }
]
#+end_src

In the aboe example the UserID is the partition key must be unique.
In a composite primary:
- first attribute: partition key (with the possibility of multiple similar partition key)
- second attribute: sort key (for similar partition key items, they must have different sort key value)

#+begin_example
Table one:
partition key: artist
Table two:
partition key: artist
sort key:song

Secondary index (table 2):
partition key: genre
album title: sort key
#+end_example

You can query the table using a secondary index which allows you to query the data using a different key. Every index belong to a table. Max 20 global index, max 5 local index 

**** DynamoDB streams
Optional feature, 24 hours version control of the data. Whenever data is added/deleted dynamo will create a record called streams. Can be linked with AWS Lambda as a trigger.

**** DynamoDB Table classes
- Std access table class
- std infrequent access table class
Each table have a class

*** OpenSearch (open source)
Textual, log/event, geospatial, time-series, json data where the shear scale and diversity to search through that amount of data requires opensearch (old elastisearch)

**** Components
- serverless(removes the complexity of provisioning)
- can be deplo
- opensearch ingestion (opensearch service domains): uses ingestion pipelines configured at the producers to move data to opensearch.

Can leverage machine learning for anomaly detection, application analytics. Uses SQL query syntax
**** Integrations
- cloudwatch
- cloudtrail
- s3/lambda/dynamodb/quicksight

Excellent for monitor/debug, store Security event monitoring (SIEM)

** Services Application Integrations

*** Autoscaling 
Allow for dynamically scale ec2 instances based on the demand. 
1. Based on a scaling (manual/dynamic/sceduled) policy with the following properties min x, desired y, max z
   * manual: manually adjust the desired capacity; auto-recovery: in the event of unhealthy instances, it will replace them
   * dynamic: scale up and down based
     - target tracking scaling: avg cpu usage, network i/o, application lb. Create/destroy instances based on the metrics
     - step scaling: uses tiers based on cloudwatch alarms to add/remove x instances (not fixed values) 
     - simple scaling: uses cloudwatch alarms (set the alarms) to scale up fixed values
     - schedule scaling: based on time of the day (more desired instances during the day and less at night) can use cron jobs
   * need to use a launch template to specify the config of the ec2 (size, ami, sg)
      
*** Elastic Loadbalancing
Allows for horizontal scaling, by forwarding incoming traffic to a specific target groups that can be across many AZ. The TG can be EC2, lambdas, Elastic Ips, or other load balancers. They always perform health checks on the target to know whether to forward to that node or not

**** Cross-zhone load balancing
has to be enabled at the route53

*** Api gateway
Allows for the following:
- Integrate various backend services into a single integrated.
- Allows for api versioning v1.1,v1.2 which have different behavior.
- request and response transformation
- security and access control
- rate limiting/throttling prevents abuse of api
- monitoring/analytics
- onboarding/docmentation (swagger)

Supported api types:
- http
- websocket
- restapi (put/get/post/delete)
-
integrates with:
- lambda
- elastic beanstalk
- ec2
- s3
- dyanmo
- RDS
- cognito

*** App flow 
fully managed integration service that enables the exchange of data between data silos and bi software.

*** AWS (simple notification services) SNS
managed aws services that delivers events (messages) from publisher to subscriber (like rabbitmq) if they are part of a topic. Max data size 256kb, if more is required use an s3 link.

**** std topic
subscirbers receives messages out of order and messages can be duplicated. can scale up
**** FIFO 
no duplications and strict ordering. 300 msg/s

Messages can be encrypted

*** AWS Simple Queue Service
loose coupling producers push messages to a queue without having to know about the consumer.
Queues: Buffer between producer/consumer
message: 

Dead letter queue: receive messages that couldn't be processed
messages are locked when processed (message locking)
**** std queues
same as sqs
**** fifo queues
same as sns

*** AWS message broker (MQ)
same as like rabbitmq but aws runs it on an aws instance.
just like aws rds but for mq

*** Eventbridge
EventBridge is a serverless service that uses events to connect application components together, making it easier for you to build scalable event-driven applications. Event-driven architecture is a style of building loosely-coupled software systems that work together by emitting and responding to events. 
The basis of EventBridge is to create rules that route events to a target.
**** event bus
An event bus is a router that receives events and delivers them to zero or more destinations, or targets.
**** pipe
A pipe routes events from a single source to a single target. The pipe also includes the ability to filter for specific events, and to perform enrichments on the event data before it is sent to the target.

*** AWS Simple Email Services (SES)
managed AWS mailing services
components:

** Services Data and ML

input data at this point is s3,rds,aurora,glue,redshift
## Data ingestion
*** Kinesis
Real time data ingestion/processing 
- video streams (video input)
- data streams (data from website/machine/etc) 
- data firehose (output not compute like ec2/lambda/emr but s3/redshift)
- data analytics
 
*** Kafka
AWS managed Apache kafka (data streaming bus like kinesis). Kafka, like kinesis, does data ingestion, data processing and data delivery.

At a high level, Apache Kafka is a distributed system of servers and clients that communicate through a publish/subscribe messaging model.
Take inputs data, place it on topics, and consumers selects topics to use.
analogy: radio station where user can tune to a specific station to get music/news(data type)

*** Glue ETL (runs pyspark code natively)
Extract from data sources, transform, load to a data target
from different sources. Uses a crawler, that output in AWS Glue data catalog and runs glue jobs
Similar to pyspark
## Data transformation
*** Elastic Map Reduce EMR 
Amazon EMR makes it simple and cost effective to run highly distributed processing frameworks such as Hadoop, Spark, and Presto when compared to on-premises deployments. 
loads data into EMR cluster (collection of ec2).Basically a managed hadoop framework. Allows for massive distributed and parallel computing.
primary node(req): manage/coordinate the cluster
core node (req): stores data + program
task node (opt): execute task only

You can use spot nodes to save money!

*** Glue Databrew
no-code program using visual interface to use glue undercover. The output can be used by athena for a data analyst or quicksight for a business analyst.
steps:
1. create projects
2. select dataset (s3, rds,etc.)
3. select recipes
4. run jobs
5. results stored in s3
# data storage/presentation 
*** Lake formation
simplified data lake creation
aggregation of meaningful dataset within a company (dynamo,redshift, s3, rds)
data can be store either in csv or analytics optimized parquet.

#+DOWNLOADED: https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2019/07/10/B.jpg @ 2024-04-19 11:53:48
[[file:Knowledge/2024-04-19_11-53-48_B.jpg]]

*** Athena
Athena helps you analyze unstructured, semi-structured, and structured data stored in Amazon S3. Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC

Serverless and pay-per-query
*** Quicksight
Amazon QuickSight is a cloud-scale business intelligence (BI) service that you can use to deliver easy-to-understand insights 
Data prep + ml insight (forecasts/anomaly)
# power of inference
*** Sagemaker
Amazon SageMaker is a fully managed machine learning (ML) service. With SageMaker, data scientists and developers can quickly and confidently build, train, and deploy ML models into a production-ready hosted environment.
workflow:
1. data ingestion
2. sagemaker notebook/data wrangler
3. training data/training jobs
4. testing/model tuning
 
*** Rekognition
Amazon Rekognition is a cloud-based image and video analysis service that makes it easy to add advanced computer vision capabilities to your applications.
object/scene detection, facial analysis/recogniztion, text in image, unsafe content
workflow:
1. upload to s3
2. triggers lambda
3. trigger call to rekognition
4. saves tags into dynamodb or s3 idk
5. can use aws ai to augment
6.
# power of inference
   
*** Polly
Amazon Polly is a cloud service that converts text into lifelike speech. You can use Amazon Polly to develop applications that increase engagement and accessibility. 
workflow:
1. upload to s3
2. triggers lambda
3. trigger call to polly (generate text to speech)
4. save audio to an s3 bucket

*** Lex
Amazon Lex is an AWS service for building conversational interfaces into applications using voice and text. With Amazon Lex, the same deep learning engine that powers Amazon Alexa.

dumps data into documentDB (mongoDB clone)

*** Comprehend
Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. 

usefull for sentiment analysis

*** Forecast
Amazon Forecast is a fully managed service that uses statistical and machine learning algorithms to deliver highly accurate time-series forecasts. 

*** Augmented AI
Amazon Augmented AI (Amazon A2I) enables you to build the workflows required for human review of ML predictions.

Essentially pay cheaply poor sods to reivew the gardbage of your ml model. allow for supervised 
*** Fraud detector
Amazon Fraud Detector is a fully managed fraud detection service that automates the detection of potentially fraudulent activities online. These activities include unauthorized transactions and the creation of fake accounts. Amazon Fraud Detector works by using machine learning to analyze your data. 
*** Transcribe
*** Translate
Amazon Translate is a neural machine translation service for translating text to and from English across a breadth of supported languages. Powered by deep-learning technologies, Amazon Translate delivers fast, high-quality, and affordable language translation. It provides a managed, continually trained solution so you can easily translate company and user-authored content or build applications

uses encoder/decoder to 
workflow:
1. upload to s3
2. triggers lambda
3. trigger call to translate 
4. save audio to an s3 bucket which can then be used by comprehend
*** Textract

A data science team is developing machine learning models and needs a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. Which AWS service offers this? ! AWS Sagemaker
A medical research company needs human review of complex medical images to train their machine learning models. Which AWS service integrates human review into machine learning applications? : aws augmented ai
An organization needs a fully managed extract, transform, and load (ETL) service to prepare and load their data for analytics. Which AWS service provides these capabilities? aws glue (not kinesis which is more for data ingestion)
A financial services firm needs to load streaming data into their AWS data stores for near real-time analytics. Which AWS service can automatically load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk? Kinesis data firehose
An insurance company needs to extract text and data from scanned documents to process insurance claims faster. Which AWS service provides optical character recognition (OCR) and machine learning to read and process any type of document? aws textract
A marketing team wants to clean and normalize their customer data without writing code. Which AWS service provides a visual interface to prepare data for analytics? glue databrew
** Services Migration and ...

** Services Management and ...

** Services Security

* Design

** Security

** Reliability

** Performance

** Cost-optimization

** Applying your desing skills
[[id:cacc3523-0db3-4184-a3fb-a4e0a320e1fb][Certs - AWS Solutions Architect - exams]]
