:PROPERTIES:
:ID:       cd0fcbf2-addf-48e6-8f15-44b95afd207d
:END:
#+title: cert - AWS Solution Architect

* Home
[[id:9d5c388a-88cd-423c-951b-5e512eae298b][Knowlege base]]
[[id:660c7092-9b98-4fa2-b271-2bbeabe1c249][Programming]]

* Main reference
** Ackronym
Secure File Transfer Protocol (SFTP)
File Transfer Protocol Secure (FTPS)
File Transfer Protocol (FTP)
Set of rules governing how files are moved between server and client running in the application layer (lv7)
Applicability Statement 2 (AS2)
[[https://docs.aws.amazon.com/]]

* Futher learning
** OSI Model
[[https://en.wikipedia.org/wiki/OSI_model][OSI model - Wikipedia]]
** File Transfer Protocol
[[https://en.wikipedia.org/wiki/File_Transfer_Protocol][File Transfer Protocol - Wikipedia]]
** VPC analogy
[[https://start.jcolemorrison.com/aws-vpc-core-concepts-analogy-guide/][AWS VPC Core Concepts in an Analogy and Guide]]
* AWS Solution Architect


** TODO Knowledge [9/9]
*** DONE Networking
CLOSED: [2024-10-14 Mon 16:34] DEADLINE: <2024-10-16 Wed>
- State "DONE"       from "TODO"       [2024-10-14 Mon 16:34]

**** Internet Gateway
**** NAT Gateway
NAT Gateway can be used to provide connectivity to the internet or external networks for AWS resources launched in a private subnet of the VPC. NAT Gateway can be of any of the following types

1. Public NAT Gateway: This can be used to provide Internet access to resources in the private subnets of the VPC. Public NAT needs to be placed in a public subnet of the VPC, and an Elastic IP address needs to be associated with this gateway.
2. Private NAT Gateway: This can be used to provide connectivity with on-premises or other VPCs from a private subnet of the VPC. No elastic IP address is required to be assigned to a private NAT Gateway.
**** Route table
to access the internet `0.0.0.0` to the ~IGW~ or ~NAT~

**** CIDR
the following five IP addresses are reserved:
- 10.0.0.0: Network address.
- 10.0.0.1: Reserved by AWS for the VPC router.
- 10.0.0.2: Reserved by AWS. The IP address of the DNS server is the base of the VPC network range plus two. For VPCs with multiple CIDR blocks, the IP address of the DNS server is located in the primary CIDR. We also reserve the base of each subnet range plus two for all CIDR blocks in the VPC. For more information, see the Amazon DNS server.
- 10.0.0.3: Reserved by AWS for future use.
- 10.0.0.255: Network broadcast address. We do not support broadcast in a VPC, therefore we reserve
this address

#+begin_src python
  """/n address calculation e.g. 10.0.0.0/27 is how many avail addresses?
  """
  n: int = 27
  answer: int = 2^(32 - n) - 5 # -5 due to AWS reserved IPs.
#+end_src

**** VPC
With Amazon Virtual Private Cloud (Amazon VPC), you can launch AWS resources in a logically isolated virtual network that you've defined. This virtual network closely resembles a traditional network that you'd operate in your own data center, with the benefits of using the scalable infrastructure of AWS.

#+DOWNLOADED: screenshot @ 2024-10-14 15:55:23
[[file:AWS_Solution_Architect/2024-10-14_15-55-23_screenshot.png]]

***** VPC Peering connection
PC peering cannot be established even if any one of the VPC subnets will be overlapping with subnets in another VPC.

**** VPC endpoints
~VPC endpoints~ in AWS when you want to privately connect your Virtual Private Cloud (VPC) to supported AWS services or other resources WITHOUT NEEDING AN INTERNET gateway, NAT gateway, VPN connection, or ~AWS Direct Connect~. VPC endpoints allow traffic between your VPC and AWS services to stay within the AWS network, enhancing security and reducing the need for public internet access.


#+DOWNLOADED: screenshot @ 2024-10-14 16:23:04
[[file:AWS_Solution_Architect/2024-10-14_16-23-04_screenshot.png]]

**** AWS Privatelink (everything else)
AWS PrivateLink is a highly available, scalable technology that you can use to privately connect your VPC to services as if they were in your VPC. You do not need to use an internet gateway, NAT device, public IP address, AWS Direct Connect connection, or AWS Site-to-Site VPN connection to allow communication with the service from your private subnets.

An example is ~AWS ECS~
WITHOUT USING INTERNET

#+DOWNLOADED: screenshot @ 2024-10-14 16:21:47
[[file:AWS_Solution_Architect/2024-10-14_16-21-47_screenshot.png]]

**** AWS Private Gateway
A Virtual Private Gateway is a key component in AWS that allows your Virtual Private Cloud (VPC) to communicate securely with an external network (like an on-premises data center) over a VPN connection or through AWS Direct Connect.

Does not support qual-cost multi-path (ECMP) VPN connections
**** VPN (on-prem)
~AWS VPN~ establishes a secure and encrypted connection between your on-premises network and your AWS VPC OVER THE PUBLIC INTERNET

AWS VPN is generally more cost-effective for smaller, less frequent data transfers and is easy to set up without any dedicated physical infrastructure.

An AWS Site-to-Site VPN has a maximum throughput of 1.25 Gbps. To scale throughput beyond 1.25 Gbps, equal-cost multi-path (ECMP) support can be used over multiple VPNs.

***** Performance optimized
****** Required site-tore vpn connection pre-requisites
- A virtual private gateway attached to the VPC
- A public IP address on the customer gateway for the on-premises network

**** Direct Connect (on-prem)
AWS Direct Connect is a cloud service solution that allows you to establish a dedicated network connection from your on-premises data center or office to AWS BYPASSING THE INTERNET

has a higher initial cost due to the dedicated line but can be more economical for large-scale data transfers and applications requiring consistent performance.

Speeds up to 1 Gbps, 10 Gbps, and up to 100 Gbps

**** Transit gateway
You can connect your virtual private clouds (VPC) and on-premises networks using a transit gateway, which acts as a central hub, routing traffic between VPCs, VPN connections, and AWS Direct Connect connections.

One of the key benefits of using a transit gateway is the ability to centralize and simplify the management of connectivity between your VPCs and on-premises networks.

#+DOWNLOADED: screenshot @ 2024-10-14 16:11:25
[[file:AWS_Solution_Architect/2024-10-14_16-11-25_screenshot.png]]

***** Secure architecture
****** control to allow/block traffice between ec2 netwrok and the transit gateway
Apply NACL rules between EC2 instances in the subnets and Transit Gateway associations to control the traffic

**** Route53
Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. You can use Route 53 to perform three main functions in any combination: domain registration, DNS routing, and health checking. =Register Domain Name=, =Route internet traffic to the resources to your doman=, =check health of resources=

***** Amazon Route 53 Routing Policies
| **Routing Policy** | **Description**                                           | **Use Case**                                             |
|---------------------|---------------------------------------------------------|---------------------------------------------------------|
| **Geoproximity**    | Routes traffic based on the geographic location of users and resources, with customizable bias. | Multi-region applications; optimizing traffic based on user location. |
| **Geolocation**     | Routes traffic based on the geographic location of users. | Serving different content based on user location; compliance with regional regulations. |
| **Weighted**        | Distributes traffic across multiple resources based on assigned weights. | A/B testing; gradual migration of traffic to new resources. |
| **Latency**         | Routes traffic to the resource that provides the lowest latency to the user. | Performance-sensitive applications requiring low response times. |

***** Record type
[[https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html][Supported DNS record types - Amazon Route 53]]
****** Alias recod
Amazon Route 53 alias records provide a Route 53–specific extension to DNS functionality. Alias records let you route traffic to selected AWS resources, such as CloudFront distributions, Elastic Load Balancing, and Amazon S3 buckets.
****** A ipv4
point to a web server e.g. 192.0.1.2
****** AAAA ipv6
point ot a web server e.g. 2001:0db8:85a3:0:0:8a2e:0370:7334
****** CAA
****** CNAME
A CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com, www.example.com or example.net) or subdomain (acme.example.com or zenith.example.org).
e.g. hostname.example.com

***** HealthCheck
****** active-active failover
 In an active-active setup, multiple nodes (servers, resources, etc.) are all active and serving traffic simultaneously. If one node fails, the remaining active nodes continue to handle traffic without interruption.
****** active-passive failover
In an active-passive setup, only one node is active and serving traffic at any given time, while the other node(s) are in a standby mode.
For configuring active-passive failover with multiple primary and secondary resources, the following setting can be done.
1. For Primary resources, create an alias record pointing to Application Load Balancer with ‘evaluate health check’ as yes.
2. For Secondary resources, create health checks for the web servers in the data centers.
3. Create two failover alias records, one for primary and one for secondary resources.

**** Lambda@edge (cloudfront only)
Lambda@Edge is a feature of Amazon CloudFront that allows you to run your Lambda functions in response to CloudFront events. It enables you to customize the content delivered by CloudFront without managing your own infrastructure.

***** When to Use Lambda@Edge
  *Content Customization**
   Modify content returned by CloudFront based on viewer requests.

  *Dynamic Content Generation**
   Generate dynamic content (e.g., user-specific data) at the edge.

  *Request and Response Manipulation**
   Change headers, rewrite URLs, or manipulate requests and responses.

  *Security Features**
   Implement authentication or token validation before requests reach the origin.

  *Real-Time Image and Video Processing**
   Perform transformations on media files (e.g., resizing images) before serving them.

  *Geolocation-Based Content Delivery**
   Deliver different content based on the geographical location of the user.

  *A/B Testing and Personalization**
   Serve different versions of content to different users for testing and personalization.

  *Custom Error Handling**
   Create customized error responses for different HTTP status codes.

  *Redirects and Rewrites**
   Set up URL redirects based on request parameters or paths.

  **Bot Protection**
  - Analyze requests to filter out bot traffic and prevent abuse.

  **Caching Strategy**
  - Implement custom caching rules based on request attributes.

  **Custom Logging and Analytics**
  - Log requests and responses for analytics or monitoring purposes.

***** Event Types
You can associate Lambda functions with various CloudFront events:

    - Viewer Request: Execute code when a viewer request is received by CloudFront.
    - Origin Request: Execute code when CloudFront forwards a request to the origin.
    - Origin Response: Execute code when CloudFront receives a response from the origin.
    - Viewer Response: Execute code just before CloudFront sends a response to the viewer.

**** Global accelerator
AWS Global Accelerator is a service in which you create accelerators to improve the performance of your applications for local and global users. Depending on the type of accelerator you choose. It provides a set of static IP addresses that act as a fixed entry point to your applications, allowing for more consistent and reliable access, regardless of where the users are located.

Users accessing the application will connect through the static IP addresses provided by Global Accelerator.
If one of the regional endpoints goes down, Global Accelerator automatically reroutes traffic to a healthy endpoint in another region, ensuring minimal disruption.
Latency is reduced as Global Accelerator uses the AWS global network to optimize the path to the nearest regional endpoint.

*** DONE Storage
CLOSED: [2024-10-14 Mon 12:36] DEADLINE: <2024-10-06 Sun>
- State "DONE"       from "TODO"       [2024-10-14 Mon 12:36]

**** EBS
An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. For current-generation volumes attached to current-generation instance types, you can dynamically increase size, modify the provisioned IOPS capacity, and change volume type on live production volumes.

both mountable and bootable. Allows for the decoupling of storage and compute just like ~ENI~ decouples networking from compute.

***** Encryption
Encrypted ~EBS~ volumes encrypts data at rest, when moving data between volume and instances. Supported accross all current generation instances types.

To encrypt an unencrypted ~EBS~ volume you restore an ~EBS~ volume from a snapshot and select the encrypted parameter and =KmsKeyId=

***** Snapshot
~EBS~ are AZ senstive so if you want to attach the ~EBS~ to and ~EC2~ in a different instance you must make a snapshot and create a volume from the snapshot. Snapshots (stored in an ~S3~) are available within an region. You can also copy the snapshot to different region.

***** Recycle bins
Recycle Bin, a data recovery feature, is a new feature introduced by AWS that enables one to restore accidentally deleted Amazon EBS snapshots and EBS-backed AMIs
***** Volume types
| EBS Volume Type            | Storage Type | Key Features                                         | Cost (USD/GB/month) | IOPS Cost (if applicable)          | IOPS                             |
|----------------------------+--------------+------------------------------------------------------+--------------------+-----------------------------------+----------------------------------|
| gp3 (General Purpose SSD)   | SSD          | Customizable IOPS, scalable performance, lower cost   | $0.08               | $0.005/IOPS (beyond 3,000 IOPS)   | 16000 IOPS (64 kib/rest 16 kib) |
| gp2 (General Purpose SSD)   | SSD          | Balanced performance, scales with volume size         | $0.10               | Included up to 3 IOPS/GB          | 16,000 IOPS      |
| io1 (Provisioned IOPS SSD)  | SSD          | High-performance, customizable IOPS                   | $0.125              | $0.065 per provisioned IOPS       | 64,000 IOPS                  |
| io2 (Provisioned IOPS SSD)  | SSD          | Higher durability, enterprise workloads, customizable | $0.125              | $0.065 per provisioned IOPS       | 256,000 IOPS               |
| st1 (Throughput Optimized)  | HDD          | High throughput for streaming workloads               | $0.045              | N/A                               | Max 500 IOPS                     |
| sc1 (Cold HDD)              | HDD          | Low-cost archival storage for infrequent access       | $0.015              | N/A                               | Max 250 IOPS                     |
| Magnetic (Deprecated)       | HDD          | Legacy option, slower, lower-cost storage             | $0.05               | N/A                               | Max 40-200 IOPS                  |

**** Instance store
Instance storage (also known as ephemeral storage) refers to directly attached, temporary disk storage that is physically located on the underlying hardware hosting an Amazon EC2 instance. Unlike Amazon EBS volumes, which persist independently of the instance lifecycle

instance stores do not persist during the stop/start of the instance only reboot

**** EFS
Amazon Elastic File System (Amazon EFS) provides serverless, fully elastic file storage so that you can share file data without provisioning or managing storage capacity and performance. They are region specific but multi-az avail. For cross region, consider ~S3~

EFS only works with Linux

***** Storage class
| Storage Class                | Description                                         | Cost                          | Use Case                                   |
|------------------------------+-----------------------------------------------------|-------------------------------|--------------------------------------------|
| EFS Standard                 | High availability and durability, frequent access   | Higher cost per GB            | Frequently accessed data, active workloads  |
| EFS Infrequent Access (IA)   | Lower storage cost, access fees per operation       | Lower cost per GB, plus access cost | Infrequently accessed data, archival, backups |


***** Types
| **Type**                  | **Category**           | **Description**                                                | **Use Case**                                   |
|---------------------------+-----------------------+----------------------------------------------------------------|------------------------------------------------|
| General Purpose            | Performance Mode       | Optimized for low-latency applications                          | Web servers, content management systems         |
| Max I/O                   | Performance Mode       | Scalable for high-throughput workloads, higher latency         | Big data, media processing                      |
| Bursting Throughput        | Throughput Mode        | Scales with file system size, suitable for variable demand     | Most applications with variable demand         |
| Provisioned Throughput     | Throughput Mode        | Configurable fixed throughput independent of storage size      | Consistent performance-critical applications     |
| Elastic Throughput         | Throughput Mode        | Automatically scales throughput with changes in workload      | Applications with fluctuating performance needs |
***** Performance optimized
****** Must be accessible by multiple ~AWS EC2~
use ~EFS~
***** Cost optimized
****** Transfer EFS data between region
Use ~AWS DataSync~ (data migration) to transfer data between ~AWS EFS~

**** FSx
Amazon FSx makes it easy and cost effective to launch, run, and scale feature-rich, high-performance file systems in the cloud. It supports a wide range of workloads with its reliability, security, scalability, and broad set of capabilities. With Amazon FSx, you can choose between four widely-used file systems: Lustre, NetApp ONTAP, OpenZFS, and Windows File Server

***** types of FSx
| **FSx Type**                | **Description**                                                  | **Use Case**                                             |
|-----------------------------|------------------------------------------------------------------|---------------------------------------------------------|
| FSx for Windows File Server  | Fully managed Windows file system with SMB protocol support      | Ideal for Windows-based applications, file sharing, and workloads needing Active Directory integration. |
| FSx for Lustre              | High-performance file system optimized for workloads requiring fast processing | Suitable for high-performance computing (HPC), machine learning, and big data analytics.           |
| FSx for NetApp ONTAP        | Fully managed NetApp ONTAP file system with support for NFS and SMB protocols | Best for enterprise applications needing advanced data management features, data protection, and scalability. |
| FSx for OpenZFS             | Fully managed OpenZFS file system providing snapshot and replication capabilities | Ideal for applications that require efficient data protection, versioning, and quick recovery.       |

| **FSx Type**                | **Latency**               | **Max Throughput**            | **Description**                                                  |
|-----------------------------|--------------------------|-------------------------------|------------------------------------------------------------------|
| FSx for Windows File Server  | Milliseconds              | Up to 2,000 MB/s             | Fully managed Windows file system with SMB protocol support.      |
| FSx for Lustre              | Sub-millisecond           | Up to 6,000 MB/s             | High-performance file system optimized for HPC and data analytics.|
| FSx for NetApp ONTAP        | Milliseconds              | Up to 2,000 MB/s             | Managed NetApp ONTAP file system with NFS and SMB support.       |
| FSx for OpenZFS             | Milliseconds              | Up to 2,000 MB/s             | Managed OpenZFS file system with snapshots and replication.       |

***** Performance optimized
****** High performance compute
Use ~AWS FSx Lustre~
**** S3
Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.

~AWS EFS~ is a nfs type storage, unlike ~AWS EBS~ an ~AWS S3~ bucket cannot be mounted to an ~AWS EC2~

***** Upload limit
5gb for a single put operration. Using multi-part you can upload up to 5tb
***** Storage classes
| **S3 Storage Class**        | **Description**                                                     | **Use Case**                                      | **Cost**                                  | **Retrieval Time**                          |
|-----------------------------+---------------------------------------------------------------------+---------------------------------------------------+-------------------------------------------+---------------------------------------------|
| S3 Standard                 | General-purpose storage for frequently accessed data                | Web applications, big data analytics              | Highest cost per GB                       | Milliseconds                                |
| S3 Intelligent-Tiering      | Automatically moves data between two access tiers                   | Unknown access patterns                           | Cost-effective for variable access        | Milliseconds to minutes (depending on tier) |
| S3 Standard-IA              | Infrequent access storage for data that is less frequently accessed | Backups, disaster recovery                        | Lower cost, retrieval fees apply          | Milliseconds                                |
| S3 One Zone-IA              | Lower-cost option for infrequently accessed data                    | Secondary backups, easily reproducible data       | Lower cost than Standard-IA               | Milliseconds                                |
| S3 Glacier                  | Archival storage for data that is rarely accessed                   | Long-term data archiving                          | Lowest storage cost, retrieval fees apply | Minutes to hours                            |
| S3 Glacier Deep Archive     | Lowest-cost archival storage for long-term data retention           | Regulatory archives, compliance data              | Lowest cost, long retrieval times         | 12 to 48 hours                              |
| S3 Outposts                 | S3 storage on-premises using AWS Outposts                           | Local data processing, hybrid cloud architectures | Pricing varies by deployment              | Milliseconds                                |
| S3 Glacier instant retrival | longterm storage but requires immedate access                       |                                                   |                                           | Milliseconds                                |

****** glacier select
because the application needs to retrieve data from Glacier. With Glacier Select, you can perform filtering directly against a Glacier object using standard SQL statements.
****** Glacier deep archive retrieval/move
Objects in Glacier Deep Archive cannot be directly moved to another storage class. To move objects from Glacier Deep Archive to different storage classes, first, need to restore them to original locations using the Amazon S3 console & then use the lifecycle policy to move objects to the required S3 Intelligent-Tiering storage class.
****** Glacier retrival expiration
after a set amount of time 1 day -> 30 days the object will expire and be removed (the original is still frozen)
****** Glacier deletion prevention
An Amazon S3 Glacier vault can be attached with one vault access policy and a vault lock policy. The Vault Access policy can be used to manage access permission to the vault. With Vault Lock Policy, no changes can be done to the policy once it's locked.
S3 Object Lock can be set in one of the two ways: =Retention Period= in which objects are locked for a specific time period and =Legal Hold= in which objects can be locked with no expiration date.
****** Glacier retrieval costs
1. No retrieval limit (default): high costs no limit
2. Free tier: retrival within the daily free tier allowance all else is rejected
3. Max retrieval rate: retrieve more data thawhat is in free tier allowance
****** glacier retrival tasks
1. std retrieval: takes up t 12 hrs to get the data (deep glacier)/ 3-5 glacier flexible
2. bulk: 5-48 hrs depending on the type of storage
3. expedited: 1-5 mins except 250mb+ objets

***** Lifecycle policy
S3 Lifecycle helps you store objects cost effectively throughout their lifecycle by transitioning them to lower-cost storage classes, or, deleting expired objects on your behalf. To manage the lifecycle of your objects, create an S3 Lifecycle configuration for your bucket. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects.

****** Transition action
Automatically move objects to different storage classes based on their age.
****** Expiration action
Delete objects after they are no longer needed, reducing storage costs.

***** Versioning
You can use S3 Versioning to keep multiple versions of an object in one bucket so that you can restore objects that are accidentally deleted or overwritten.
****** Deleting versioned objects
 DELETE API call on the object does not delete the actual object, but places delete marker on the object. delete marker, Amazon S3 behaves as though the object has been deleted (even though it has not been erased) and returns a 404 error.

To permanently delete versioned objects, you must use =DELETE Object versionID=. =DELETE= isn't enough
***** Replicating object within/accross region
You can use replication to enable automatic, asynchronous copying of objects across Amazon S3 buckets. Similar to an ~AWS RDS~ =snapshot= effectively provides a copy of the data.

****** Cross-Region Replication (CRR)
You can use CRR to replicate objects across Amazon S3 buckets in different AWS Regions

****** Same-Region Replication (SRR)
You can use SRR to copy objects across Amazon S3 buckets in the same AWS Region.


***** Multipart Upload (reliability)
Ideal for high-latency, unreliable networks where you are uploading very large files (hundreds of MBs to GBs) and want to optimize for upload reliability and speed.

 A file is broken into multiple parts (up to 10,000 parts, each between 5 MB and 5 GB), which can be uploaded independently. Once all parts are uploaded, they are combined into a single object. This reduces the risk of failure on large uploads, as a failure will only affect a single part, which can be retried without restarting the whole upload.

***** Transfer acceleration (speed focus)
Useful when latency is the primary concern

Optimizes the speed of uploads by leveraging the AWS global network of edge locations (Amazon CloudFront) to accelerate the transfer of data to S3.
Instead of sending data directly to an S3 bucket, the data is routed through the nearest Amazon CloudFront edge location, which then transfers it over AWS's high-speed backbone network to the S3 bucket.

***** Event notification
Amazon S3 Event Notifications enable you to automatically trigger specific actions when certain events occur in an S3 bucket.

****** Object Created
When a new object is uploaded (e.g., =s3:ObjectCreated:Put=, s3:ObjectCreated:Post, etc.).
****** Object Deleted
When an object is deleted (e.g., =s3:ObjectRemoved:Delete=).
****** Object Restore Completed
When an archived object is restored from S3 Glacier or S3 Glacier Deep Archive.
****** Object Tagging
When tags are added or updated on an object.

****** Destination target
~Lambda~, ~SQS~, ~SNS~

***** Access point
AWS S3 Access Points are a feature that simplifies managing access to shared datasets in Amazon S3. They provide a more flexible and controlled way to access S3 buckets, especially for applications with complex access patterns

Example S3 data lake where you'd want an access point for reads, upload and external access.

***** Object lock
feature that helps you enforce retention policies on objects stored in S3, preventing them from being deleted or overwritten for a specified period.
****** Governance Mode
Allows users with specific permissions to overwrite or delete the objects but prevents all others from doing so.
****** Compliance Mode
Objects cannot be deleted or overwritten by any user, including the root user in your AWS account, for the duration of the retention period.
****** Retention period
You can specify a retention period for each object when it is uploaded or by applying a policy. The retention period can range from a minimum of 1 day to several years.
****** Legal hold
In addition to retention periods, you can place a legal hold on an object, preventing it from being deleted or overwritten indefinitely, regardless of the retention mode or period.
***** ACL + Bucket policy
****** ACL
Use ACLs for simple access control scenarios where you want to manage access at the object level and don't need complex conditions.
****** Bucket policy
Use Resource Policies for more sophisticated access control requirements, especially when you need to implement fine-grained permissions or allow access from other AWS accounts. Similar to ~AWS IAM~ policies.

#+begin_src json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:root"
      },
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::my-bucket/*"
    }
  ]
}
#+end_src

***** Encryption
| **Encryption Method**                       | **Description**                                                   | **Key Management**                          | **Access Control**                             | **Use Case**                             |
|---------------------------------------------|-------------------------------------------------------------------|---------------------------------------------|------------------------------------------------|------------------------------------------|
| **SSE-S3**                                  | Amazon S3 manages encryption and decryption using its own keys.  | Managed by Amazon S3                        | Basic IAM permissions for S3                   | General-purpose use, no key control needed  |
| **SSE-KMS**                                 | Uses AWS KMS for managing encryption keys, offering more control. | Customer Master Keys (CMKs) in AWS KMS     | Fine-grained control via IAM and KMS policies   | Sensitive data requiring access control   |
| **SSE-C**                                   | Users provide their own encryption keys for S3 to use.           | Managed by the user                         | User-defined policies for key access            | Complete control over encryption keys     |


***** Security Optimized
****** Grant temporary access to a user with no AWS credentials
Use ~S3~ presigned URLs
****** read and write permissions to a single user on specific S3 object
Use ~S3 ACLs~
****** Enforce all objects uploaded to an s3 bucket are encrypted at rest
S3 Bucket Policies can be used to enforce that all objects uploaded to an S3 bucket are encrypted at rest by

***** Performance optimized
****** upload large files over a high-latency network to S3
Use ~S3 Transfer Acceleration~
****** Get a notification when an object is modified. Solution must be scalable w/reprocessing capabilities
Use Amazon S3 Event Notifications with Amazon EventBridge. EventBridge can reprocess an event if there's any error during processing.

***** Cost optimized
****** determine which object aren't accessed regularly
~S3 Analytics~  Storage Class Analysis helps analyze S3 object access patterns to determine when to transition objects to less expensive storage classes.

**** AWS Cloudfront
Amazon CloudFront speeds up distribution of your static and dynamic web content, such as .html, .css, .php, image, and media files. When users request your content, CloudFront delivers it through a worldwide network of edge locations that provide low latency and high performance.

***** Cache

***** custom error page
Put the static error pages in an S3 bucket. Create custom error responses for the HTTP 5xx status code in the CloudFront distribution.

***** cloudfront functions
CloudFront Functions is a lightweight serverless compute service that allows you to run JavaScript code at the edge to manipulate requests and responses. It is designed for simple, fast processing.

****** lambda@edge vs cloudfront function
| Feature                   | Lambda@Edge                             | CloudFront Functions                   |
|---------------------------|-----------------------------------------|----------------------------------------|
| **Execution Points**      | Viewer Request, Viewer Response,        | Viewer Request, Viewer Response        |
|                           | Origin Request, Origin Response         |                                        |
| **Use Cases**             | Dynamic content, A/B testing, auth,    | URL rewriting, header manipulation,    |
|                           | personalization                         | caching                                |
| **Language Support**      | Multiple (Node.js, Python, Java, Ruby) | JavaScript (ECMAScript 2020)          |
| **Execution Duration**    | Up to 30 seconds                       | Up to 1 millisecond                    |
| **Cost**                  | Based on requests and execution time    | Based on number of invocations         |

****** CORS
Only with json
CORS can be enabled with the following settings,
1. Access-Control-Allow-Origin
2. Access-Control-Allow-Methods GET/POST
3. Access-Control-Allow-Headers
***** no access to s3 bucket
****** origin access identity(legacy)/conrol (OAI/OAC)
When you first set up an Amazon S3 bucket as the origin for a CloudFront distribution, you grant everyone permission to read the files in your bucket which allows anyone to access your files either through CloudFront or using the Amazon S3 URL.
If you use CloudFront signed URLs or signed cookies to restrict access to files in your Amazon S3 bucket, you probably also want to prevent users from accessing your Amazon S3 files by using Amazon S3 URLs. If users access your files directly in Amazon S3, they bypass the controls provided by CloudFront signed URLs or signed cookies.
***** Allows access to s3 bucket
****** signed urls
CloudFront signed URLs allow you to restrict access to individual files. Signed URLs require you to change your content URLs for each customer access.

****** signed cookies
CloudFront Signed Cookies allow you to control access to multiple content files and you don’t have to change your URL for each customer access.
**** EBS vs EFS vs FSx
| **Service**        | **Use Case**                                       | **Access Type**                     | **Key Characteristics**                             |
|--------------------|----------------------------------------------------|-------------------------------------|----------------------------------------------------|
| Amazon EBS         | Block storage for EC2 instances                     | Attached to a single instance       | Low-latency access, persistent storage for databases |
| Amazon EFS         | Shared file storage                                 | Concurrent access by multiple instances | High availability, scales automatically with demand  |
| Amazon FSx         | Fully managed file systems with specific features   | Shared access with advanced capabilities | Windows compatibility, high-performance workloads   |

**** Storage Gateway (on-prem data backup)
AWS Storage Gateway is a service that connects an on-premises software appliance with cloud-based storage to provide seamless and secure integration between your on-premises IT environment and the AWS storage infrastructure in the AWS Cloud.

Storage Gateway is mainly used for moving backups to the cloud, using on-premises file shares backed by cloud storage, and providing low-latency access to data in AWS for on-premises applications.
#+DOWNLOADED: screenshot @ 2024-10-14 11:50:18
[[file:AWS_Solution_Architect/2024-10-14_11-50-18_screenshot.png]]

***** Direct Connect
AWS Direct Connect links your internal network to the Amazon Web Services Cloud. By using AWS Direct Connect with Storage Gateway, you can create a connection for high-throughput workload needs, providing a dedicated network connection between your on-premises gateway and AWS.

SKIPS THE INTERNET

***** VPN
secure connection over the internet


**** AWS Backup
AWS Backup is a fully-managed service that makes it easy to centralize and automate data protection across AWS services, in the cloud, and on premises. Using this service, you can configure backup policies and monitor activity for your AWS resources in one place. It allows you to automate and consolidate backup tasks that were previously performed service-by-service, and removes the need to create custom scripts and manual processes.

#+DOWNLOADED: screenshot @ 2024-10-14 11:58:59
[[file:AWS_Solution_Architect/2024-10-14_11-58-59_screenshot.png]]


*** DONE Compute
CLOSED: [2024-10-14 Mon 13:30] DEADLINE: <2024-10-06 Sun>
- State "DONE"       from "TODO"       [2024-10-14 Mon 13:30]
**** EC2
Amazon Elastic Compute Cloud (Amazon EC2) provides on-demand, scalable computing capacity in the Amazon Web Services (AWS) Cloud.

***** instance type
| Instance Type    | Category            | vCPUs | Memory (GiB) | Network Performance | Storage    | Use Case                                            |
|------------------+---------------------+-------+--------------+---------------------+------------+----------------------------------------------------|
| t3.micro         | On-Demand            | 2     | 1            | Up to 5 Gigabit     | EBS only   | General purpose, low-cost, burstable workloads      |
| r6g.large        | Memory Optimized     | 2     | 16           | Up to 10 Gigabit    | EBS only   | Memory-intensive applications                       |
| c6g.large        | Compute Optimized    | 2     | 4            | Up to 12 Gigabit    | EBS only   | Compute-heavy tasks, high-performance computing     |
| p4d.24xlarge     | Accelerated Compute  | 96    | 1152         | 4 x 100 Gigabit     | NVMe SSD  | Machine learning, HPC, and deep learning workloads  |
| i3en.xlarge      | Storage Optimized    | 4     | 32           | Up to 25 Gigabit    | NVMe SSD  | I/O intensive tasks, databases, large storage needs |
| m6i.large        | General Purpose      | 2     | 8            | Up to 12.5 Gigabit  | EBS only   | Balanced compute, memory, and networking            |

***** AMI
An Amazon Machine Image (AMI) is an image that provides the software that is required to set up and boot an Amazon EC2 instance. There are public/private and shared.

***** Key Pair
A key pair, consisting of a public key and a private key, is a set of security credentials that you use to prove your identity when connecting to an Amazon EC2 instance. For Linux instances, the private key allows you to securely SSH into your instance.

***** Lifecycle

#+DOWNLOADED: screenshot @ 2024-10-10 20:10:30
[[file:AWS_Solution_Architect/2024-10-10_20-10-30_screenshot.png]]

***** EBS
used for persistent data
****** volumes
can be attached to the ec2
****** snapshot
point in time saving

***** ELB
To balance the incoming requests to a number of servers

***** autoscaling
Scale based on schedule/cloudwatch alarms/automatic

***** Elastic IP
dedicated AWS IP

***** Launch template
You can use an Amazon EC2 launch template to store instance launch parameters so that you don't have to specify them every time you launch an Amazon EC2 instance. For example, you can create a launch template that stores the AMI ID, instance type, and network settings that you typically use to launch instances. Usefull for autoscaling groups.

#+DOWNLOADED: screenshot @ 2024-10-10 20:17:27
[[file:AWS_Solution_Architect/2024-10-10_20-17-27_screenshot.png]]

***** Cluster placement group

Cluster: same rack
partition: same az
spread: same region

#+DOWNLOADED: screenshot @ 2024-10-10 20:21:14
[[file:AWS_Solution_Architect/2024-10-10_20-21-14_screenshot.png]]

***** Pricing model
| Pricing Model         | Description                                            | Use Cases                                 | Discount             |
|-----------------------+--------------------------------------------------------+-------------------------------------------+----------------------|
| On-Demand             | Pay as you go, no upfront commitment                   | Short-term, spiky workloads               | No discount           |
| Spot                  | Bid for unused capacity at up to 90% discount          | Batch jobs, fault-tolerant workloads      | Up to 90% off         |
| Reserved Instances    | Commitment to 1 or 3 years with upfront payment options| Predictable, long-term workloads          | Up to 75% off         |
| Savings Plans         | Flexible plan based on committed usage (dollars/hour)  | Predictable spend but variable workload   | Up to 72% off         |
| Dedicated Hosts       | Physical server dedicated to your use                  | Compliance, licensing needs               | No discount           |
| Dedicated Instances   | Isolated EC2 instances on dedicated hardware           | Single-tenant environments                | No discount           |
| EC2 Fleet             | Mix of On-Demand, Spot, and Reserved Instances         | Large-scale workloads, cost optimization  | Varies                |

***** High performing architecture
****** update the server
write scripts which can be added to the User Data section when the instance is launched.

**** EC2 Image builder
EC2 Image Builder is a fully managed AWS service that helps you to automate the creation, management, and deployment of customized, secure, and up-to-date server images.


#+DOWNLOADED: screenshot @ 2024-10-10 20:34:31
[[file:AWS_Solution_Architect/2024-10-10_20-34-31_screenshot.png]]

***** Golden image
An EC2 golden image is a pre-configured Amazon Machine Image (AMI) that serves as a template for launching EC2 instances.

The process is as follow:
- Choose a base image for your customizations.
- Add to or remove software from your base image.
- Customize settings and scripts with build components.
- Run selected tests or create custom test components.
- Distribute AMIs to AWS Regions and AWS accounts.

**** Elastic Network Interface
An elastic network interface is a logical networking component in a ~VPC~ that represents a virtual network card.


#+DOWNLOADED: screenshot @ 2024-10-10 20:40:37
[[file:AWS_Solution_Architect/2024-10-10_20-40-37_screenshot.png]]


decouple the compute from the networking. Each ec2 starts with an ~ENI~ called the primary which cannot be detached. Additional are called secondary eni, can be used for a low budget high availability solution. secondary can be detached.

**** BeanStalk (fullstack)
AWS Elastic Beanstalk is a Platform as a Service (PaaS) that simplifies deploying, managing, and scaling web applications and services.
With AWS Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications.


***** Target Audience
Developers and teams looking to deploy web applications quickly without managing underlying infrastructure but still want flexibility and control over the resources.

**** Lightsail (VPS)
Amazon Lightsail is the easiest way to get started with Amazon Web Services (AWS) for anyone who needs to build websites or web applications. It includes everything you need to launch your project quickly

***** Target Audience
Developers, small businesses, or non-technical users who want a simple, low-cost VPS (virtual private server) with minimal AWS knowledge.

**** ECS
Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service that helps you to more efficiently deploy, manage, and scale containerized applications.

***** Services
A service definition defines which task definition to use with your service, how many, which cluster,

***** Launch types
- ec2: still need to manage the ec2 infra
- fargate: aws manages the infra

***** Security optimized
****** ECS is not getting any status information back from the container agent
- IAM role used to run EC2 instanced does not have =ecs:poll= action in its policy
- Interface VPC endpoint in not configured for ECS servcie ~AWS PrivateLink~
**** EKS
Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that eliminates the need to install, operate, and maintain your own Kubernetes control plane on Amazon Web Services (AWS).

***** Worker nodes
****** self-managed
provision ec2 instaces yourself, you install and configure kubernetes bare metal. This also includes the version update
****** managed node group
Automates the provisioning and lifecycle management of ec2 nodes, a more streamlined way to manage lifecycle
****** fargate
AWS manages all

**** ECR
Amazon Elastic Container Registry (Amazon ECR) is a fully managed container registry offering high-performance hosting, so you can reliably deploy application images and artifacts anywhere.

there are 2 kinds public and private.
Can integrate with ~AWS Codecommit~ (aws version of github) and ~AWS codebuild~ (aws version of pipelines)

**** App runner
AWS App Runner is an AWS service that provides a fast, simple, and cost-effective way to deploy from source code or a container image directly to a scalable and secure web application in the AWS Cloud. You don't need to learn new technologies, decide which compute service to use, or know how to provision and configure AWS resources.

An even more streamlined version. push your code or docker image and aws will do the rest

**** Batch
AWS Batch helps you to run batch computing workloads on the AWS Cloud. Batch computing is a common way for developers, scientists, and engineers to access large amounts of compute resources. AWS Batch removes the undifferentiated heavy lifting of configuring and managing the required infrastructure, similar to traditional batch computing software.

#+DOWNLOADED: screenshot @ 2024-10-10 21:02:01
[[file:AWS_Solution_Architect/2024-10-10_21-02-01_screenshot.png]]

**** Lambda
With AWS Lambda, you can run code without provisioning or managing servers. You pay only for the compute time that you consume—there's no charge when your code isn't running.

***** Updating the function
when changing the code then uploading the new version to AWS Lambda there will be a brief perios =less than a minute= where the old lambda may be served.
***** performance optimized
****** mySQL DB invoke lambda
- Ensure that the Aurora MySQL DB cluster has an IAM Role which allows it to invoke Lambda functions.
- Configure the Aurora MySQL DB cluster to allow outbound connections to the Lambda function.
**** AWS Serverless Application Model (SAM)
~AWS Serverless Application Model~ (AWS SAM) is an open-source framework for building serverless applications using infrastructure as code (IaC).

SAM template -> s3 bucket -> cloudFormation -> stack/chage set

Here's an example: build and deploy serverless applications using AWS Lambda, Amazon API Gateway, and Amazon DynamoDB
***** Serverless Application repository
create SAM -> publish to AWS Serverless application repo (kinda like ~AWS ECR~)

**** Amplify (frontend)
AWS Amplify is best suited for developers looking to build serverless web and mobile applications quickly, leveraging managed services for hosting, APIs, authentication, and storage.

Use AWS Amplify to develop and deploy cloud-powered mobile and web applications. Amplify provides frontend libraries, UI components, and backend building for fullstack applications on AWS.

**** Outpost
AWS Outposts brings native AWS services, infrastructure, and operating models to virtually any data center, co-location space, or on-premises facility. You can use the same services, tools, and partner solutions to develop for the cloud and on premises.

TL;DR: bring AWS hardware to your local on prem

**** ECS/EKS anywhere
AWS ECS Anywhere and AWS EKS Anywhere are extensions of the respective container orchestration services that allow you to run your containerized applications on your own hardware, including on-premises servers or edge locations.

Both ECS Anywhere and EKS Anywhere allow you to run containerized applications on your own physical hardware,

**** vmware
So many company uses vmware for their virtualization needs that AWS want to help them move to AWS.

#+DOWNLOADED: screenshot @ 2024-10-14 12:58:51
[[file:AWS_Solution_Architect/2024-10-14_12-58-51_screenshot.png]]


*** DONE Database
CLOSED: [2024-10-10 Thu 19:53] DEADLINE: <2024-10-06 Sun>
- State "DONE"       from "TODO"       [2024-10-10 Thu 19:53]
**** RDS
Amazon Relational Database Service (Amazon RDS) is a web service that makes it easier to set up, operate, and scale a relational database in the cloud. It provides cost-efficient, resizeable capacity for an industry-standard relational database and manages common database administration tasks. Amazon Aurora is a fully managed relational database engine that's built for the cloud and compatible with MySQL and PostgreSQL.

***** Parameter groups
ou manage your DB engine configuration through the use of parameters in a DB parameter group. DB parameter groups act as a container for engine configuration values that are applied to one or more DB instances

***** Transparent Data encryption
Amazon RDS supports TDE (Transparent Data Encryption) with Oracle and SQL servers. With TDE, data is automatically encrypted before it is written to storage and automatically decrypted when data is read from the storage.

***** Deployment
****** Single-AZ Database
One region (single AZ), low costs, high latency and no redundancy if the AZ goes down. Can have multiple read replicas
****** Multi-AZ
Multiple AZ, can only have one writer but can have as many reader. SINGLE REGION
****** Read replicas
create read-only replicas of your db in one or more region (cross region).
****** Aurora global db
replicate data across multiple AWS regions allowing for low latency
****** Serverless
variable workloads + unpredictable traffic patterns
***** Blue/Green
A blue/green deployment copies a production database environment to a separate, synchronized staging environment. By using Amazon RDS Blue/Green Deployments, you can make changes to the database in the staging environment without affecting the production environment.
***** Storage devices
| Storage Type       | Description                                                       | Use Case                                  |
|--------------------+-------------------------------------------------------------------+-------------------------------------------|
| General Purpose SSD| Cost-effective storage that balances price and performance.        | Ideal for most workloads.                 |
| Provisioned IOPS   | High-performance SSD with provisioned IOPS for intensive workloads.| Applications needing consistent IOPS.     |
| Magnetic (Previous)| Older generation storage, slower and cheaper.                     | Not recommended for new deployments.      |
| Aurora Storage     | Scalable, SSD-based storage automatically managed by Aurora.      | Specific to Amazon Aurora RDS.            |

***** Primary database is failing
What would happen to the db if the primary DB instance fails? the canonical name record will be changed from the primary to the standby
***** Security Optimization
****** sensitive healthcare data
use ~AWS KMS~ to encrypt data at rest to meet secure and compliance regulations
***** Performance Optimization
****** transaction-heavy application
Configure ~AWS RDS~ with provision IOPS storeage for consisten and fast i/o performance

**** RDS Aurora
Amazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL.

#+DOWNLOADED: screenshot @ 2024-10-08 20:24:39
[[file:AWS_Solution_Architect/2024-10-08_20-24-39_screenshot.png]]

***** Amazon Aurora Parallel Query
Amazon Aurora Parallel Query is a powerful feature designed to enhance the performance of complex analytical queries in Amazon Aurora. By distributing query workloads across multiple compute resources, it allows for faster query execution and improved resource efficiency, making it ideal for analytical and reporting applications.

***** Resiliency optimization
****** Financial institution
Amazon Aurora Global Databases is an ideal recommendation. This feature enables cross-region disaster recovery and data replication, providing a higher level of resilience compared to single-region deployments.

**** RDS Proxy
AWS RDS Proxy is a fully managed, highly available database proxy service that sits between your application and your Amazon RDS or Amazon Aurora databases. It helps improve the scalability, performance, and security of database-intensive applications by managing database connections efficiently.

Quite usefull is your ec2s connection to your db are serverless functions to process a user demand.

#+DOWNLOADED: screenshot @ 2024-10-20 20:15:16
[[file:AWS_Solution_Architect/2024-10-20_20-15-16_screenshot.png]]

***** connection pooling
allows for multiple application request to share the same connection
***** Automatic failover
automatically routes connections to the standby database.

***** Resiliency optimization
****** database connectivity issues
proxy can managed database connections and reduce the burden on the database server.

**** redshift
Amazon Redshift is a fast, fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to efficiently analyze all your data using your existing business intelligence tools. It is optimized for datasets ranging from a few hundred gigabytes to a petabyte or more and costs less than $1,000 per terabyte per year

#+DOWNLOADED: screenshot @ 2024-10-08 21:09:12
[[file:AWS_Solution_Architect/2024-10-08_21-09-12_screenshot.png]]

_analytical operation not transactional_

***** Serverless (cost saving)
provisioned capacity is really expensive, serverless allows us to pay for compute used only. calculated in RPU(16 gb of ram) between 8 -> 512. min 32 rpu for 128 tb+

***** Spectrum
Redshift Spectrum is not a query performance enhancer for data stored in Redshift. It is used for querying data directly in S3 files

***** AQUA (Advanced Query Accelerator)
Using AQUA, customers can boost the query performance by 10X. It resolves network bandwidth and memory processing (CPU) bottleneck, low cost, and is easy to deploy.
***** Datashare
A datashare is a unit of sharing data that can be created for sharing data in Amazon Redshift with the users in the same or different accounts. It integrates with AWS IAM which provides a controlled way of sharing data with specific users in different accounts. For sharing Redshift data between cross-accounts, both the source and destination clusters must be encrypted and in the same region

***** Cost optimized
****** Share ~AWS Redshift data~ between accts in the same region
Create a datashare from the redshift console and authorize specific accounts for access
****** reduce storage costs
~AWS RedShift Spectrum~ allows users to run queries against exabytes of data in Amazon S3 without having to load and transform any data.

**** DynamoDB
Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. like rds but for nosql.

***** Streams
A DynamoDB Stream is an ordered flow of information about changes to items in an Amazon DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table
***** Standard access table class
***** Infrequent access table class

***** Resiliency optimization
****** high available serverless deployment
Implementing automated snapshots and enabling cross-region snapshot copy in Redshift Serverless are effective strategies for data backup and recovery
****** High demand period
Implement ~AWS dynamoDB~ global tables to provide multi-region, fully replicated data for high avail + disaster recovery.
***** Performance optimization
****** real-time analytics or large dataset
configure ~AWS redshift~ with RA3 nodes to leverage managed storage and high-performance computing for large dataset
****** throttling noticed
enable autoscaling for the underlying dynamodb table

**** DynamoDB accelerator (cache)
DynamoDB Accelerator (DAX) is an in-memory caching service for Amazon DynamoDB that helps improve the performance of read-intensive applications. It acts as a fully managed cache that is tightly integrated with DynamoDB, delivering fast, low-latency data access.

cluster that sits between the request and dynamodb. milli second response but expensive. Scalable up to 10 nodes and fully managed.

**** Opensearch
Amazon OpenSearch Service is a managed service that makes it easy to deploy, operate, and scale OpenSearch, a popular open-source search and analytics engine.

OpenSearch Advantage: OpenSearch is designed for full-text search and handles unstructured data like logs, documents, and text-based data. It can efficiently index, search, and retrieve data using advanced search algorithms.

Use OpenSearch for:

    Full-text search and unstructured data.
    Real-time data analytics and log analysis. (cloudwatch)
    High query throughput with low-latency results.
    Scalable search for large datasets.

#+DOWNLOADED: screenshot @ 2024-10-08 21:31:06
[[file:AWS_Solution_Architect/2024-10-08_21-31-06_screenshot.png]]

***** Serverless
***** ultrawarm
UltraWarm provides a cost-effective way to store large amounts of read-only data on Amazon OpenSearch Service. Standard data nodes use "hot" storage, which takes the form of instance stores or Amazon EBS volumes attached to each node. Hot storage provides the fastest possible performance for indexing and searching new data.

***** Performance optimization
****** requires low-latency
Get the biggest instance you can afford
***** Security optimization
****** Only to be accessible internally
use ~AWS VPC endpoints~. It allows the media company to keep all traffic between their AWS resources and OpenSearch within the AWS network, avoiding exposure to the public internet.

Honestly reminds me of bugsnap

**** ElastiCache
Amazon ElastiCache is a web service that makes it easy to set up, manage, and scale a distributed in-memory data store or cache environment in the cloud.

primarily to cache data between your server and your db (just like redis)

#+DOWNLOADED: screenshot @ 2024-10-08 21:40:31
[[file:AWS_Solution_Architect/2024-10-08_21-40-31_screenshot.png]]

***** Serverless
allows you to scale

***** Redis pub/sub message system
you send a message to a specific channel not knowing who, if anyone, receives it. The people who get the message are those who are subscribed to the channel.
***** Encryption at rest
***** Global datastore
allows for caching globally

***** Cost optimized
****** variable traffic patterns
For predictable workloads, using reserved nodes in ElastiCache for Redis can be more cost-effective than on-demand pricing.
**** MemoryDB
Essentially AWS redis db (cache + db). Can deploy in a cluster, same behavior as ~AWS RDS~

**** DocumentDB
AWS version of MongoDB

#+DOWNLOADED: screenshot @ 2024-10-08 21:51:16
[[file:AWS_Solution_Architect/2024-10-08_21-51-16_screenshot.png]]

***** DocumentDB Global Cluster
you can have secondary clusters in different region

***** Cost optimization
Leverage ~AWS DocumentDB~ with reserved instances to reduce costs for predictable long term usage.

**** keyspaces (cassandra)
AWS managed Apache cassandra

Apache Cassandra is an open-source, distributed NoSQL database management system designed to handle large amounts of structured and unstructured data across many commodity servers, providing high availability with no single point of failure.

use CQL (cassandra query language)

#+DOWNLOADED: screenshot @ 2024-10-08 21:56:57
[[file:AWS_Solution_Architect/2024-10-08_21-56-57_screenshot.png]]

***** Serverless

***** Multi-region replication
can replicate data closest to your users

***** Performance optimized
****** low-latency responses for high velocity data ingestion
use Provisioned IOPS storage

**** Neptune (graphdb)
Amazon Neptune is a fast, reliable, fully managed graph database service

#+DOWNLOADED: screenshot @ 2024-10-08 22:00:10
[[file:AWS_Solution_Architect/2024-10-08_22-00-10_screenshot.png]]

***** serverless

***** Performance optimized
****** low-latency responses for high velocity data ingestion
use Provisioned IOPS storage

**** QLDD (bitcoin/ledger)
Amazon Quantum Ledger Database (Amazon QLDB) is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log owned by a central trusted authority.

***** Performance optimized
****** low-latency responses for high velocity data ingestion
use Provisioned IOPS storage

**** Timestream
Amazon Timestream for LiveAnalytics is a fast, scalable, fully managed, purpose-built time series database that makes it easy to store and analyze trillions of time series data points per day.

#+DOWNLOADED: screenshot @ 2024-10-08 22:04:41
[[file:AWS_Solution_Architect/2024-10-08_22-04-41_screenshot.png]]

***** Dynamic schema
***** Serverless
***** Data lifecycle
Old IOT data isn't as usefull as newer one

***** Performance Optimized
****** handling and querying
AWS Timestream is specifically designed for time-series data and offers a memory store for recent data and a magnetic store for older data

*** DONE Application integration
CLOSED: [2024-10-07 Mon 11:06] DEADLINE: <2024-10-06 Sun>
- State "DONE"       from "TODO"       [2024-10-07 Mon 11:06]
**** Autoscaling
***** Scaling policy
minimum, desired and max instances
- manual: all operations are done manually
- dynamic: 3 types:
****** target tracking policy
This policy adjusts the number of instances in a way that keeps a specific metric, such as CPU utilization or request count, close to a target value.
=ASGAverageCPUUtilization= for an ~AWS EC2~, network =ASGAverageNetworkIn=
****** simple scaling policy
Must set and use ~AWS cloudwatch~ alarm (high usage/low usage). you can control the scaling process e.g. 50-70 add 2 ~EC2~, 85-100 add 5 ~EC2~
****** step scaling policy
caling adjusts the capacity based on step adjustments instead of a target

- scheduled
For predicatable and known loads

***** EC2 specific
you must specify a launch template for the new ~EC2~ to use. e.g. what ~AWS EC2 AMI/size/SG/IAM/EBS volume~ to use.

***** Misc
****** cooldown period
The cooldown period is the amount of time to wait for a previous scaling activity to take effect is called the cooldown period.
****** update EC2 isntance part of a scaling gruppen
Put the instance in Standby mode. Post upgrade, move instance back to InService mode. It will be part of the same auto-scaling group
****** autoscaling termination policy
Termination policy is used to specify which instances to terminate first during scale-in
****** warm pool
 Auto Scaling Warm Pool is a collection of pre-initialized EC2 Instances sitting along with your Auto Scaling Group
****** Hibernation
Hibernation of the Amazon EC2 instance can be used in the =case of memory-intensive applications= or if =applications take a long time to bootstrap=. Hibernation pre-warms the instance, and after resuming it, it quickly brings all application processes to a running state. When an instance is hibernated, the Amazon EC2 instance saves all the content of the instance memory RAM to Amazon EBS volumes. Any root EBS volumes or attached EBS volumes are persisted during hibernation.
****** autoscaling for dbs (data loss) post shutdown
Adding Lifecycle Hooks to the Auto Scaling group puts the instance into a wait state before termination. During this wait state, you can perform custom activities to retrieve critical operational data from a stateful instance. The Default Wait period is 1 hour.

***** Secure architecture
****** EC2 configuration
use ~AWS AMI~ to ensure the same configuration access
***** Cost optimized
****** Burst of usage
using Spot Instances with EC2 Auto Scaling is an effective cost-optimization strategy. Spot Instances allow users to take advantage of unused EC2 capacity at a significantly lower price compared to On-Demand pricing.
****** Spot instance in non-prod
use parameter to set the =OnDemandPercentageAboveBaseCapacity= to 0 (so about 0 use all spot instances)
***** Cost Performance
****** Identify Public and Private IP
 Using a Curl or Get Command to get the latest meta-data from http://169.254.169.254/latest/
meta-data/

**** ELB
Elastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets and routes traffic only to the healthy targets.

***** Listeners
A listener is a process that checks for connection requests, using the protocol and port that you configure. Before you start using your Application Load Balancer, you must add at least one listener.

***** Target group (which servers request are forwarded too)
Target groups route requests to individual registered targets, such as EC2 instances, using the protocol and port number that you specify.

***** Application load balancer
Supports Http/https and allows for advanced rerouting. e.g. http -> https redirect,
Can allow filtering on GET/POST request or host header rules e.g. blog.website.com -> redirect to ~Target group~ named blog containing dedicated ~AWS EC2~ or path e.g. blog.website.com/store
mostly used for web apps (direct traffic to the right EC2)
****** Server Name Identification
ALB supports Server Name Indication (SNI), enabling hosting multiple domain names with different TLS certificates behind a single ALB. With SNI, multiple certificates can be associated with listeners in ALB, enabling each web application to use separate certificates.
***** Network load balancer
Supports TCP, UDP, and TLS

The NLB creates and attaches ~ENIs~ (virtual network interfaces) to the subnets you specify during setup. These ENIs represent the entry points for traffic in each Availability Zone.

***** Misc
****** Cross-zone load balancing
If not enabled it only goes to a specific zone

***** Resilient architecture
****** Handle AZ failures
Configure the ALB to distribute traffic across multiple ~AWS EC2~ instances in different AZ =us-east-1a=, =us-east-1b=. Reminder the ~AWS Subnet~ can only be in one specific ~AZ~
***** Secure architecture
****** 2 sites in one ec2/group of ec2
 Use multiple TLS certificates on ALB using Server Name Indication (SNI).

**** API Gateway
Amazon API Gateway enables you to create and deploy your own REST and WebSocket APIs at any scale. You can create robust, secure, and scalable APIs that access Amazon Web Services or other web services, as well as data that’s stored in the AWS Cloud.

- Backend integration complexity
- api management and deployment (versioning)
- request and response transformation
- security and access control ~AWS Cognito~
- Rate limiting and throttling
- Monitoring and analytics
- onboarding and documentation e.g. ~swagger docs~

Supports:
- http api (cheapest)
They are geared toward use cases where speed and cost efficiency are critical, and fewer features are needed.
They provide basic authentication (JWT or OAuth) and routing functionality but lack the deeper feature set of REST APIs.
HTTP APIs are better suited for lightweight, straightforward API designs and serverless functions.
- REST
Request/Response Validation: You can automatically validate API requests and responses, ensuring that the correct data structure is used.
Transformation: You can transform data formats (e.g., from XML to JSON) directly within the API Gateway.
Authentication/Authorization: More advanced security integrations (AWS IAM, Cognito, etc.) are available out of the box.
Caching: Built-in caching mechanisms reduce backend load, but caching increases costs.
Integration Flexibility: REST APIs allow for more complex integration with AWS services.
- websocket

***** Integration sources
- http
- VPC link
- lambda function from another acct

***** Resilient architecture
****** financial sector
~AWS API Gateway~ in conjunction with ~AWS Lambda~ for backend processing is a highly resilient configuration. This allows for automatic scaling to handle varying loads and ensures fault tolerance, as Lambda functions can be automatically triggered from different Availability Zones.

***** Cost optimized
****** http vs rest
choosing HTTP APIs in AWS API Gateway is a prudent decision. HTTP APIs are a more cost-effective option than REST APIs, especially for applications with a high number of API calls. They offer lower cost per million invocations

***** Secure architecture
****** encrypted traffic
Implement SSL/TLD termination on the ELB to ensure encrypted data transmission between clients and the load balancer

**** Appflow
Amazon AppFlow is a fully managed API integration service that you use to connect your software as a service (SaaS) applications to AWS services, and securely transfer data. Use Amazon AppFlow flows to manage and automate your data transfers without needing to write code.

***** Resilient architecture
****** potential connection lost
Use ~AWS S3~ storage as a buffer storage, ensuring data is not lost in case of intermittent connectivity issue.

***** Performant architecture
****** High-volume, real-time data transfer analytics
Integrating ~AWS AppFlow~ with ~Amazon Redshift~ for direct data transfer is an effective strategy for a media company needing high-performance data transfer for real-time analytics.

**** SNS (notification)
Amazon Simple Notification Service (Amazon SNS) is a managed service that provides message delivery from publishers to subscribers (also known as producers and consumers). Publishers communicate asynchronously with subscribers by sending messages to a topic

#+DOWNLOADED: screenshot @ 2024-10-06 18:58:27
[[file:AWS_Solution_Architect/2024-10-06_18-58-27_screenshot.png]]

***** First In First Out Topic
You can use Amazon SNS FIFO (first in, first out) topics with Amazon SQS FIFO queues to provide strict message ordering and message deduplication. max 300 messages/seconds

***** Standard Topic
Main issue: messages may show up more than once and out of order. It has high throughput tho

***** Misc
****** Batching
you can batch 1-10 messages per request. max size 256kb but can send 2gb (~S3~ link)

***** Calid delivery protocols (for receiving notifications)
- lambda
- http(s)
- email
- sms
- sqs
- Email/email-json (not xml)

***** Resilient architecture
****** high-volume order processing and notifications
Integrating Amazon SNS with Amazon SQS is a robust strategy for building a resilient notification architecture. This combination allows for decoupling the order processing system from the notification system. When order volumes are high, Amazon SQS can buffer and manage the messages, ensuring that the notification system is not overwhelmed.

***** High-Performing architecture
****** maximize the performance and scalability for broadcasting
Using a fan-out pattern with Amazon SNS topics and multiple Amazon SQS queues is an effective strategy for achieving high performance and scalability when broadcasting alerts to a large audience.

#+DOWNLOADED: screenshot @ 2024-10-07 13:15:50
[[file:AWS_Solution_Architect/2024-10-07_13-15-50_screenshot.png]]

**** STS
AWS STS (Security Token Service) is a service that enables the creation of temporary, limited-privilege credentials for securely accessing AWS resources.

- STS enables you to request temporary, limited-privilege credentials
- enables users to assume role
- by default, ~AWS STS~ is available as a global service

**** SQS
Amazon Simple Queue Service (Amazon SQS) is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS moves data between distributed application components and helps you decouple these components.

***** Policy
Using SQS policies, you can grant permissions to different SQS actions to other AWS accounts.

***** Autoscaling
You can use the number of messages stored in an SQS queue as an indicator of the amount of work
waiting in line for eventual processing within an Auto Scaling Group comprised of a variable number of
EC2 instances.

***** High-Performing architecture
****** max throughput
Set up ~AWS SQS~ with batch message processing to increase the number of messages processed by actions.
****** fleet ~EC2~ to accept video uploads and fleet ~EC2~ to process the video
Create an SQS queue to store the information for Video uploads. Spin up the processing servers
via an Autoscaling Group. Ensure the Group scales based on the size of the queue.

#+DOWNLOADED: screenshot @ 2024-10-06 19:18:12
[[file:AWS_Solution_Architect/2024-10-06_19-18-12_screenshot.png]]

*****  Standard queues
Standard queues ensure at-least-once message delivery, but due to the highly distributed architecture, more than one copy of a message might be delivered, and messages may occasionally arrive out of order.

***** FIFO
FIFO (First-In-First-Out) queues have all the capabilities of the standard queues, but are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated.

***** Dead letter queues
Amazon SQS supports dead-letter queues (DLQs), which source queues can target for messages that are not processed successfully. DLQs are useful for debugging your application because you can isolate unconsumed messages to determine why processing did not succeed.

***** Visibility timeout
When a consumer receives a message from an Amazon SQS queue, the message remains in the queue but becomes temporarily invisible to other consumers. This temporary invisibility is controlled by the visibility timeout

***** Polling
****** Short(latency optimized)
 Short polling immediately returns a response to the consumer, regardless of whether there are messages available in the queue. low latency (usefull for real-time and frequent)

 Short polling is good when you want low-latency message retrieval and can tolerate frequent requests even when the queue might be empty.

****** Long (cost optimized)
 Long polling waits until a message is available in the queue before returning a response, or until a specified timeout is reached. Reduces the number of API request
***** Misc
****** ApproximateNumberOfMessagesVisible
pproximateNumberOfMessagesVisible describes the number of messages available for retrieval. It can be used to decide the queue length.
****** ApproximateNumberOfMessagesNotVisible
ApproximateNumberOfMessagesNotVisible measures the number of messages in flight.

***** Resilient architecture
****** enhance resilience of order processing system
Enable the Dead letter Queue to manage undeliverable events

**** AWS MQ
Amazon MQ is a managed message broker service that makes it easy to set up and operate message brokers in the cloud. Amazon MQ provides interoperability with your existing applications and services.

| Resource type              | Amazon SNS | Amazon SQS | Amazon MQ |
|----------------------------+------------+------------+-----------|
| Synchronous                | No         | No         | Yes       |
| Asynchronous               | Yes        | Yes        | Yes       |
| Queues                     | No         | Yes        | Yes       |
| Publisher-subscriber messaging | Yes     | No         | Yes       |
| Message brokers            | No         | No         | Yes       |

***** Performant architecture
****** high-volume/real-time transaction data
use ~AWS MQ~ with dedicated broker instances to provide dedicated resources for high traffic

**** AWS Eventbridge
Amazon EventBridge is a serverless event bus service that makes it easy to connect your applications with data from a variety of sources. EventBridge delivers a stream of real-time data from your own applications, software-as-a-service (SaaS) applications, and AWS services and routes that data to targets such as AWS Lambda.

#+DOWNLOADED: screenshot @ 2024-10-06 20:08:41
[[file:AWS_Solution_Architect/2024-10-06_20-08-41_screenshot.png]]

***** High-Performing architecture
****** processing high-volume, real-time events
real-time analytics platform dealing with a large volume of events, using ~AWS EventBridge~ in conjunction with ~AWS Lambda~ allows for efficient and scalable event processing

***** Resilient architecture
****** enhance resilience of order processing system
using the default event bus, Enable the Dead letter Queue to manage undeliverable events

**** AWS SES (email)
Amazon Simple Email Service (SES) is an email platform that provides an easy, cost-effective way for you to send and receive email using your own email addresses and domains.

***** Verified identies
Same thing as supabase + resend integration to reduce span
***** Misc
****** Solution that ensures high deliverability rates and efficient handling of bounce and complaint notifications.
A dedicated IP pool allows the firm to manage its own email sending reputation, which is crucial for ensuring that their emails are not marked as spam and that they reach their intended recipients

**** AWS Step functions
AWS Step Functions is a serverless orchestration service that lets you integrate with AWS Lambda functions and other AWS services to build business-critical applications.
AWS Step Functions coordinate application components using visual workflows. It keeps track of all task and events in an application. ~AWS SQS~ requires you to implement your own application-level tracking

***** Resilient architecture
****** In a transaction system
Use the built-in retry policies

***** High-Performing architecture
****** Parallel
You can use parallel state to process multiple files/process concurently

***** Cost optimized
****** filter events
sing AWS EventBridge with a default event bus and applying filtering rules, the application can ensure that only relevant events are processed

#+DOWNLOADED: screenshot @ 2024-10-06 19:59:01
[[file:AWS_Solution_Architect/2024-10-06_19-59-01_screenshot.png]]

***** Misc
****** step func vs SQS
 Although Amazon SQS and Step Functions both help in some sort of orchestration. Amazon SQS doesn’t have the capability to let you track all the tasks and events of your application.

**** Workflow services
Use step function most of the time unless you require external signal to interact within the process, or start child processes.

Need to use python, go, javascript, etc.

***** Resilient architecture
****** Complex system
By incorporating parallel processing of transactions in AWS Simple Workflow Service, the financial services company can ensure that the failure or delay of one task does not halt or significantly impact the entire process.

**** Maanged Workflow for Apache Airflow (MWAA)
Amazon Managed Workflows for Apache Airflow is a managed orchestration service for Apache Airflow that you can use to setup and operate data pipelines in the cloud at scale.

***** Resilient architecture
****** enhance resilience
By configuring AWS MWAA to use ~Amazon RDS~ Multi-AZ deployments for its metadata database, the Solutions Architect can ensure that the database, which is central to the operation of Apache Airflow, remains highly available and resilient to failures.
*** DONE Data ML
CLOSED: [2024-09-30 Mon 12:59] DEADLINE: <2024-09-29 Sun>
- State "DONE"       from "TODO"       [2024-09-30 Mon 12:59]
**** kinesis (Data ingestion)
- video streams
Inputs (video's feed)
Output can be ~AWS Rekognition~, ~AWS S3~, ~AWS Sagemaker~
- data streams
Inputs any data stream
You can use Amazon Kinesis Data Streams to collect and process large streams of data records in real time.
Outputs can be ~AWS EC2~, ~AWS Lambda~, ~AWS EMR~, ~Kinesis Data Analytics~ (mostly other compute)
- data firehose
Inputs any data stream
~Amazon Kinesis Data Firehose~ cannot directly send data logs to Amazon Redshift but needs to first store in the Amazon S3 bucket & then it copies data to Amazon Redshift.
Outputs can be ~AWS S3~, ~AWS Redhshift~. The main difference between that and ~data streams~ is that the outputs are data repositories
- data analytics (queries on the data before it hits storage)
***** Performant architecture
****** thousand of IoT logs comming for redshift analysis
Create an Amazon Kinesis Data Firehose Delivery Stream, save compressed data in the Amazon S3 bucket and then copy data to Amazon Redshift.
~data stream~ isn't required as the extra processing or storing of logs is extra.

**** Kafka
Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that makes it easy for you to build and run applications that use Apache Kafka to process streaming data. High data ingestion, processing and delivery. Can be serverless.
~Apache Kafka~ is a distributed queue system decoupling producer and consumers. Queues are split into partitions which can be consumed using the partition key. Partitions hosted on a machine are called brokers

**** glue ETL
~AWS Glue~ is a serverless data integration service that makes it easy for analytics users to discover, prepare, move, and integrate data from multiple sources. You can use it for analytics, machine learning, and application development.
Defined datastore, create a crawler, populates glue data catalog

**** Elastic Map Reduce (EMR)
Amazon EMR is a managed cluster platform that simplifies running big data frameworks like Apache Hadoop and Apache Spark on AWS.  There is ~EMR Studio~ which allows for collaboration
~Apache Hadoop~ stores data accross several nodes in a cluster, process data accross multiple nodes, and then stores the results
~Apache Spark~ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.
Can be hosted on ~EC2~, ~EKS~ or ~serverless~
***** performing architecture
****** long term/cost-effective that can handle any amount of data
Use Amazon EMR with Apache Spark & data stored in Amazon S3 bucket.

**** glue Databrew
NoCode application of ~AWS Glue~

**** lake formation
AWS Lake Formation is a managed service that makes it easy to set up, secure, and manage your data lakes. Lake Formation helps you discover your data sources and then catalog, cleanse, and transform the data. Any source can be ~nosql~, ~s3~, ~redshift~, ~sql~

Lake Formation collects and catalogs data from databases and object storage, moves the data into your new Amazon Simple Storage Service (S3) data lake, uses ML algorithms to clean and classify the data, and secures access to the sensitive data using granular controls at the column, row, and cell-levels.

**** Athena
Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to setup or manage, and you pay only for the queries you run.

**** Quicksight
Serverless data visuliaztion engine for interactive dashboarding pulling data from:
- s3
- athena
- RDS
- redshift
- aurora
- glue

**** Sagemaker
Build, train, and deploy machine learning models (image classification, object detection, regression, clustering/grouping, etc.) at scale.
[[https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html][Algos - Amazon SageMaker]]
Process:
- Data ingestion (~s3~ bucket)
- Data preparation and exploration
- Model training
- Model Evaluation + parameter tuning
- Model deployment
AWS process:
Pull data stored in ~s3~ using ~AWS Glue~ for ETL into sagemaker for:
- exploratory data analysis
- data cleaning
- building model
- deploy model

**** rekognition
AWS ML service that analyze and intrepret images and videos. Can be used for content moderation. Comes with a probability score
AWS process:
- user upload image from website to s3
- trigger's ~AWS lambda~
- AWS rekognition dumps metadata into ~AWS dynamoDB~
- low probability images can leverage ~AWS augmented AI~ for human to review machine learning predictions

**** Polly
Turn text into speech.
AWS process:
[[file:~/Documents/zettelkasten/data/image/cert/satty-20240925-14:02:00.png][polly]]

**** Lex
AWS chatbot (think alexa)
provides advanced deep learning functionalities of automatic speech recognition (ASR) for converting speech to text and natural language understanding (NLU) to recognize the intent of the text?
**** Comprehend
Natural language processing and text analysis.
***** Personally identifiable information e.g. credit card numb
**** forecast(NoCode)
Delivers forecasts on time series data (sales, website traffic, etc.)
**** Augmented AI
Integrate human reviewers to review AI's prediction.
Can be used for ~AWS Trasnlate~ for low confidence translation
~AWS rekognition~ for low confidence image label/sentiment/etc.

~AWS Mechnical Turk (MTurk)~ a virtual workforce that is paid per assignemtn to do this if you don't have the manpower to man A2I.
**** Fraud detector (NoCode)
Build, deploy, manage fraud detection model. Usefull for payment fraud detection.
[[file:~/Documents/zettelkasten/data/image/cert/fraudDectection.png][Fraud detection]]
**** transcribe
Speech to text. The opposite of ~AWS polly~
**** translate
AWS Google translate. Usefull for a single chatbot using multiple languages.
Can upload custom terminology to augment the translator.
**** textract
extract text from scanned forms.
Can extract:
- text
- forms
- tables
- signatures

*** DONE Migration/transfer
CLOSED: [2024-09-30 Mon 11:25] DEADLINE: <2024-09-29 Sun>
- State "DONE"       from "TODO"       [2024-09-30 Mon 11:25]
**** Intro
To migrate from on prem to AWS
- assess and create inventory
- categorize the items
- determine AWS cloud services
- plan migration
- execute the migration
**** migration hub
AWS Migration Hub (Migration Hub) provides a single place to discover your existing servers, plan migrations, and track the status of each application migration.
Connect migration tools to migration hub, migrate using the tools, and group servers as applications
***** Cost Optimization
****** During the migration
By using ~AWS Migration Hub~ to monitor the migration progress, the Solutions Architect can identify any delays or issues that might lead to extended migration timelines and, consequently, higher costs.
****** Assess on-premise infrastructure
Leverage ~AWS Apllication Discovery~ to identify over-provisioned resouces and recommend rightsizing before migration to AWS.
***** Security optimization
****** Security of sensitive data
Implement ~AWS IAM~ roles + policies to control access to ~AWS Migration Hub~ and resources being migrated.
***** Reliability/Resilience optimization
****** Critical continuity
By using ~AWS Migration Hub~ to plan and execute a sequential migration, starting with the most critical application tiers, the Solutions Architect can ensure that the most essential services remain available during the migration process.
**** application discovery service
~AWS Application Discovery Service~ helps you plan your migration to the AWS cloud by collecting usage and configuration data about your on-premises servers and databases
The ~AWS Application Discovery Agent~ (Discovery Agent) is software that you install on on-premises servers and VMs targeted for discovery and migration. Has to be installed on every server
~Application Discovery Service Agentless~ Collector(Agentless Collector) is a virtual appliance that you install in your on-premises VMware environment. Can only be used for =VMware environments=
The data gathered will be stored in an ~AWS S3 bucket~ and can be access by ~AWS Athena~ ~AWS migration hub~ ~AWS database migration services~
***** Performance Optimization
****** on-premises infra bottleneck
~AWS Application discovery~ can be used to identify underutilized resources and optimize for performance
****** post migration
implement ~AWS Global Accelerator~ to optimize network paths and improve game server performance post migration
***** Security Optimization
****** AWS Application services
store ~AWS Application discovery~ service gathered data into ~AWS S3~ with encryption enabled by ~AWS KMS~
**** application migration service
AWS Application Migration Service ~AWS MGN~ is a highly automated lift-and-shift (rehost) solution that simplifies, expedites, and reduces the cost of migrating applications to AWS.
Setup service, import inventory, replicates and syncs the data, test, and cutover
***** Security Optimization
****** Sensitive data
Use ~AWS KMS~ to encrypt data before migrating it with ~AWS MGN~. Ensures data is protect both in transit and at rest
**** database migration service
AWS Database Migration Service (AWS DMS) is a cloud service that makes it possible to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores.
Allows for schema conversion e.g. mySql to postgresQL ~DMS schema conversion tool~
replication task (on prem source endpoint -> target AWS cloud endpoint) using DMS EC2 replication instance
- Full load (requires downtime)
- Full load + CDC
- CDC only
Allows for continuous data
***** Performance Optimization
****** Want best
Consider the use case ~AWS aurora~ for sql, ~AWS redshift~ for data warehousing, ~AWS DynamoDB~ for k/v + noSql
***** Reliability/Resilience
****** high resilience/fault tolerence during and after migration process
~AWS DMS~ with multi-az deployment for target db
***** Cost Optimization
****** startup unsure about AWS db costs
Use ~AWS Aurora serverless~ to automatically scale capacity and minimize costs > ~AWS RDS~ w/ reserved instance pricing model

**** Elastic disaster recovery
AWS Elastic Disaster Recovery (AWS DRS) minimizes downtime and data loss with fast, reliable recovery of on-premises and cloud-based applications =GCP= =Azure= using affordable storage, minimal compute, and point-in-time recovery. Can also be used on AWS fro region to region
Main issue without ~AWS EDR~ for on-prem services expensive (requires duplicate infra on stby), maintenance + skilled personel intensive.
Data is replicated from on-prems to AWS, using an ~AWS EC2~ and stores it on ~AWS EBS Volumes~
Allows for real time sync and point-in-time recovery, automated DR drills
***** Cost Optimization
****** network design strategy
AWS Direct Connect establishes a dedicated network connection between the company's infrastructure and AWS, which can significantly reduce data transfer costs during disaster recovery operations.
****** EBS
~AWS EBS~ with snapshot lifecycle policies to automate creation/deleition + costs
****** network design
~AWS Direct Connect~ to establish a dedicated network connection, reducing data transfer costs during disaster recovery operations
***** Performance Optimization
****** Minimal downtime and quick recovery (data storage)
Leverage ~AWS EDR~ and ~AWS FSx Lustre~
**** AWS Mainframe modernization
AWS Mainframe Modernization helps you modernize your mainframe applications to AWS managed runtime environments. It provides tools and resources to help you plan and implement migration and modernization.
You can =refactor= using ~AWS blu age~ or =replatform= with ~AWS Micro focus~
***** Cost Optimization
****** reduce costs by modernizing their legacy mainframe systems
Migrate the mainframe applications to a serverless architecture using ~AWS Lambda~
***** Reliability/Resilience
implement ~AWS mainframe modernization~ with multi-az deployment for migrated applications

**** Datasync (Mass data migration)
AWS DataSync is an online data transfer and discovery service that simplifies data migration and helps you quickly, easily, and securely transfer your file or object data to, from, and between AWS storage services.
An agent must be deployed on prem then ~AWS DataSync discovery~ provides recommendations
Can also be used to transfer large amount of data between AWS region
***** Cost Optimization
****** updates only
Implement incremental data transfer with ~AWS Datasync~ to reduce volume of data transferred
***** Performance Optimization
****** During transfer
~AWS DataSync~ ability to perform parallel transfers and multipart uploads to Amazon S3 is particularly beneficial for large files.
**** AWS Transfer Family ()
AWS Transfer Family is a secure transfer service that enables you to transfer files into and out of AWS storage services.
Can be internal and also connect cloudwatch to check for what files get moved
AWS Transfer Family functions somewhat like an external drive in the sense that it allows external clients, partners, or users to access files in Amazon S3 using familiar protocols like SFTP, FTPS, or FTP
***** Security Optimization
****** strict security protocol
~AWS transfer family~ with MDA for secure access control during file transfer
**** AWS Snow family
The AWS Snow Family is a service that helps customers who need to run operations in austere, non-data center environments, and in locations where there's no consistent network connectivity. Can handle petabytes depending on the snow.
Snowball edge
snowcone
snowball: CPU optimized 104vCPU, Storage optimized 210TB NVME/80TB HDD
snowmobile: exabyte scale data migration
***** Cost Optimization
****** Large scale data transfer (several petabytes)
Use ~AWS Snowmobile~ for one-time, large-scale data transfer

*** DONE Management/Governance
CLOSED: [2024-10-02 Wed 20:41] DEADLINE: <2024-09-29 Sun>
- State "DONE"       from "TODO"       [2024-10-02 Wed 20:41]
**** cloudformation
***** drift detection
AWS CloudFormation Drift Detection can be used to detect changes made to AWS resources outside the CloudFormation Templates. AWS CloudFormation Drift Detection only checks property values explicitly set by stack templates or by specifying template parameters.

***** Parameters
Allow users to input custom values when creating or updating a stack, making templates more flexible and reusable.
#+BEGIN_SRC yaml
Parameters:
  InstanceTypeParameter:
    Type: String
    Default: t2.micro
    AllowedValues: [t2.micro, m1.small, m1.large]
    Description: Enter instance type (e.g., t2.micro)
#+END_SRC

***** Deletion
To delete an RDS and S3 bucket but to still keep the data set the =DeletionPolicy= on the RDS to snapshot and S3 to retain.

***** Mappings
Define sets of static values that are mapped to keys, which can be referenced within the template.
#+BEGIN_SRC yaml
Mappings:
  RegionToAMI:
    us-east-1:
      AMI: "ami-0ff8a91507f77f867"
    us-west-2:
      AMI: "ami-0a8e758f5e873d1c1"
#+END_SRC

***** Conditions
Define conditional logic based on input parameters or other conditions, controlling when certain resources are created or updated within the stack.
#+BEGIN_SRC yaml
Conditions:
  CreateProdResources: !Equals [ !Ref EnvType, prod ]
#+END_SRC

***** Resources
Define the AWS resources that make up your stack.
#+BEGIN_SRC yaml
Resources:
  MyEC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      InstanceType: !If [CreateProdResources, "m1.small", !Ref InstanceTypeParameter]
      ImageId: !FindInMap [RegionToAMI, !Ref "AWS::Region", AMI]
#+END_SRC

***** Mapping
Mappings section matches a key to a corresponding set of named values
#+BEGIN_SRC yaml
"Mappings" : {
  "RegionMap" : {
    "us-east-1"      : { "HVM64" : "ami-0ff8a91507f77f867"},
    "us-west-1"      : { "HVM64" : "ami-0bdb828fd58c52235"},
    "eu-west-1"      : { "HVM64" : "ami-047bb4163c506cd98"},
    "ap-southeast-1" : { "HVM64" : "ami-08569b978cc4dfa10"},
    "ap-northeast-1" : { "HVM64" : "ami-06cd52961ce9f0d85"}
  }
}
#+END_SRC

***** Outputs
Define values that are returned by the stack once it's created or updated.
#+BEGIN_SRC yaml
Outputs:
  WebsiteURL:
    Description: URL of the website
    Value: !GetAtt WebsiteBucketWebsiteURL
#+END_SRC

***** cfn-init (retrieve metadata)
helper script is used to retrieve and interpret resource metadata from the =AWS::CloudFormation::Init= key.

***** cfn-hup (update)
helper script checks for any updates to the metadata. If there are any changes, it executes custom hooks.

***** cfn-signal ()
helper script can be used to signal CloudFormation to indicate if software or application is successfully updated on an Amazon EC2 instance.

***** cfn-get-metadata
helper script helps to retrieve metadata

***** StackSets
AWS CloudFormation StackSets extends the capability of stacks by allowing you to create, update, or delete stacks across multiple accounts and AWS Regions with a single operation.

***** Nested stacks
As your infrastructure grows, common patterns can emerge in which you declare the same components in multiple templates.

terraform modules

***** Change sets
Change sets allow you to preview how proposed changes to a stack might impact your running resources

**** Cloud Development Kit (CDK)
Because writting cloudformation template by hand is a pain.

The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation.

=cdk synth= synthetise the cloudformation templates
=cdk deploy= deploys the formations to AWS.
**** Cloudwatch
Amazon CloudWatch monitors your Amazon Web Services (AWS) resources and the applications you run on AWS in real time. You can use CloudWatch to collect and track metrics, which are variables you can measure for your resources and applications.

The CloudWatch home page automatically displays metrics about every AWS service you use. You can additionally create custom dashboards to display metrics about your custom applications, and display custom collections of metrics that you choose.

You can collect system-level metrics from on-prems and view alongside AWS metrics. using ~AWS Cloudwatch agent~

***** Metric
data such as latency, cpu load, etc. from your applications

***** Alarms
Alarms from metrics. Usefull for custom alarms which can be fed into ~AWS SNS~, then you can trigger a ~AWS Lambda~. Services like ~AWS autoscaling~ works out of the box.

***** Logs
All the logs generated from your application

***** Events
usefull for usage with ~AWS EventBridge~

**** x-ray
~AWS X-Ray~ is a service that collects data about requests that your application serves, and provides tools that you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization.

**** AWS Health Dashboard
General dashboard informing AWS customers about ongoing issues that AWS services/region are experiencing.

**** Prometheus
Amazon Managed Service for Prometheus is a serverless, Prometheus-compatible monitoring service for container metrics that makes it easier to securely monitor container environments at scale.

Open source solution for ~AWS Cloudwatch~. Collects your metrics

**** Grafana
Amazon Managed Grafana is a fully managed and secure data visualization service that you can use to instantly query, correlate, and visualize operational metrics, logs, and traces from multiple sources.

Open source solution for advanced analytics and visualization platform.

**** Trusted advisor
Trusted Advisor draws upon best practices learned from serving hundreds of thousands of AWS customers. Trusted Advisor inspects your AWS environment, and then makes recommendations when opportunities exist to save money, improve system availability and performance, or help close security gaps.

Mostly recommendations (but you have to pay for them)

The key words is that its across cost savings, performance, security and fault tolerance

**** Launch Wizard
AWS Launch Wizard helps you reduce the time that it takes to deploy well knows application (~AWS EKS~, ~SAP~, ~MS Active Dir~)and domain-controller solutions to the cloud.

**** Compute Optimizer
AWS Compute Optimizer recommends optimal AWS compute resources for your workloads.

~AWS EC2~, ~AWS EBS~, Fargate

**** AWS Organization
AWS Organizations helps you centrally manage and govern your environment as you grow and scale your AWS resources. Using Organizations, you can create accounts and allocate resources, group accounts to organize your workflows, apply policies for governance, and simplify billing by using a single payment method for all of your accounts.

You can only have one root and one management account. The later creates the policies, invites organizations, applying policies, etc.

***** Organization Units
An AWS Organization has the below hierarchy of Organizational Units (OUs): Root -> organization_OU (e.g. company a) -> Dev_OU
Project_OU is attached to an SCP that prevents users from deleting VPC Flow Logs. Dev_OU has an SCP that allows the action of "ec2: DeleteFlowLogs".

***** Service Control Policies (SCP)
Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for the IAM users and IAM roles in your organization. Can apply to a specific acct, org, etc.

DOES NOT AFFECT USERS/ROLES IN MANAGEMENT ACCTS

SCPs do not grant permissions to the IAM users and IAM roles in your organization. Only defines what you are allowed todo within that account. e.g. in the dev_OU you shouldn't be spinning large ec2.

***** Resource based policies
Resource-based policies are used to specify users who can access resources and what actions they can perform on the specified resources. These policies cannot be used to share resources with AWS Organizations.

**** Control tower
AWS Control Tower is a service that enables you to enforce and manage governance rules for security, operations, and compliance at scale across all your organizations and accounts in the AWS Cloud.

best thought as an AWS acct orchestrator

***** Landing Zone
A landing zone is a well-architected, multi-account environment that's based on security and compliance best practices. Creates two AWS Organizations organizational units (OUs): Security, and Sandbox (optional), contained within the organizational root structure. Creates or adds two shared accounts in the Security OU: the Log Archive account and the Audit account.

***** Controls/guardrails
 is a high-level rule that provides ongoing governance for your overall AWS environment. It's expressed in plain language. Three kinds of controls exist: preventive, detective (reports but doesn't stop you), and proactive(does not provision). It uses ~AWS Org SCP~
Detective controls detect specific events when they occur and log the action in CloudTrail.

***** Account Factory
An Account Factory is a configurable account template that helps to standardize the provisioning of new accounts with pre-approved account configurations.

**** System manager
Use AWS Systems Manager to organize, monitor, and automate management tasks on your AWS resources.

***** Operation manager
Use Incident Manager, a capability of AWS Systems Manager, to manage incidents occurring in your AWS hosted applications.

***** Application manager
 Application Manager aggregates operations information from multiple AWS services and AWS Systems Manager capabilities to a single AWS Management Console.

***** Parameter Store
key/value pair for secret or
b**** service catalog

***** Change manager
Simplify how your team requests, approves, implements, and reports on operational changes. Manage changes to your application configuration and infrastructure, both in AWS and on premises. Can specify blackout days (no changes)

***** Node Management
AWS Systems Manager provides the following capabilities for accessing, managing, and configuring your managed nodes. A managed node is any machine configured for use with Systems Manager in a hybrid and multicloud environment.

Feels like terraform should control said changes. I do see it being usefull for DB changes.

**** Service Catalog
AWS Service Catalog enables IT administrators to create, manage, and distribute portfolios of approved products to end users, who can then access the products they need in a personalized portal. Typical products include servers, databases, websites, or applications that are deployed using AWS resources

**** License Manager
~AWS License Manager~ indeed enables organizations to track both ~AWS-provided licenses~ and custom licenses (3rd party) procured independently. It provides visibility into license usage, helps in controlling usage to ensure compliance with licensing terms, and offers features like License Manager rules to set up licensing rules.

does not automatically purchase or allocate additional licenses when usage exceeds predefined thresholds.

**** Proton
AWS Proton creates and manages standardized infrastructure and deployment tooling for developers and their serverless and container-based applications.

I dare say that terraform modules does that

**** Resource group and tag manager
AWS Resource Explorer is a resource search and discovery service. With Resource Explorer, you can explore your resources, such as Amazon Elastic Compute Cloud instances, Amazon Kinesis streams, or Amazon DynamoDB tables, using an internet search engine-like experience.

**** Resilience hub
AWS Resilience Hub helps you proactively prepare and protect your AWS applications from disruptions. AWS Resilience Hub provides resiliency assessment and validation to help you identify and resolve issues before releasing applications into production.

Acts as an overseer

***** Recovery point objective (RPO)
The maximum acceptable amount of time since the last data recovery point. This determines what is considered an acceptable loss of data between the last recovery point and the interruption of service.

***** Recovery time objective (RTO)
The maximum acceptable delay between the interruption of service and restoration of service. This determines what is considered an acceptable time window when service is unavailable.

**** Resource Explorer
Simply search and discovery of AWS resources accross regions

**** Resource Access Manager
Resource Access Manager (RAM) can be used to share resources with other accounts. With RAM, resources created in one account can be shared with other accounts eliminating admin overhead for managing these resources.

It leads to cost-saving as resources are created in one account and shared with all other accounts. In AWS Organizations, resources created in a member account or management account can be directly shared with other member accounts in the Organizations.

*** DONE Security
CLOSED: [2024-10-06 Sun 16:46] DEADLINE: <2024-09-29 Sun>
- State "DONE"       from "TODO"       [2024-10-06 Sun 16:46]
**** IAM
AWS Identity and Access Management (IAM) is a web service for securely controlling access to AWS services. With IAM, you can centrally manage users, security credentials such as access keys, and permissions that control which AWS resources users and applications can access.
Least Privilege Principle

***** User
An IAM user is an entity that you create in your AWS account. The IAM user represents the human user or workload who uses the IAM user to interact with AWS resources. A IAM user consists of a name and credentials.

***** Group
An IAM user group is a collection of IAM users.

***** Role
An IAM role is an IAM identity that you can create in your account that has specific permissions.  a role is intended to be assumable by anyone who needs it.

***** Policy
Dictate permission a user has access to.

**** IAM Identity Center (SSO)
IAM Identity Center provides one place where you can create or connect workforce users and centrally manage their access to all of their AWS accounts and applications. Workforce users benefit from a single sign-on experience and can use the AWS access portal to find all their assigned AWS accounts and applications.

***** Secure architecture
****** Give access to AWS partner network (APN) member
Create a cross account IAM Role and share the ARN with APN
****** on-premise self-service application to aws cloud
Use ~AWS IAM Identity Center~ to sign-on users defined in the Corporate Directory on-premises with SAML 2.0 based identity federation for accessing the AWS applications


**** Cognito
Amazon Cognito handles user authentication and authorization for your web and mobile apps. With user pools, you can easily and securely add sign-up and sign-in functionality to your apps.

***** User pool
An Amazon Cognito user pool is a user directory for web and mobile app authentication and authorization. A user pool adds layers of additional features for security, identity federation, app integration, and customization of the user experience.

The user are given a token.

***** Identity pool
An Amazon Cognito identity pool is a directory of federated identities that you can exchange for AWS credentials. Identity pools generate temporary AWS credentials for the users of your app

Allows for the user to access AWS resources (upload to an S3)

**** Directory Service
MS Active Dir: Directory service created by microsoft which enable administrator to managed permissions + access to different services/applications.

AWS Directory Service provides multiple ways to set up and run Microsoft Active Directory with other AWS services such as Amazon EC2, Amazon RDS for SQL Server, FSx for Windows File Server, and AWS IAM Identity Center.

TL;DR AWS Managed Microsoft AD managed accross multiple availability zones.

***** Simple AD
Does not integrate with on-prems but easy integration
***** Managed Ms AD
actual implementation, on AWS, of MS AD
***** AD Connnector
if you have ms AD on prems you don't have to get a duplicate

**** Verified permissions
Amazon Verified Permissions is a scalable, fine-grained permissions management and authorization service for custom applications built by you.
attribute-based access control (ABAC) to manage permission as opposed to Role-baed access control (RBAC) used in k8s

***** Differences between IAM
IAM Permissions are for AWS resources (e.g., S3, EC2, Lambda).
AWS Verified Permissions are for application-level access control, extending beyond AWS services to control actions within an application.

**** CloudTrail
With AWS CloudTrail, you can monitor your AWS deployments in the cloud by getting a history of AWS API calls for your account, including API calls made by using the AWS Management Console, the AWS SDKs, the command line tools, and higher-level AWS services. You can also identify which users and accounts called AWS APIs for services that support CloudTrail, the source IP address from which the calls were made, and when the calls occurred.

TL;DR: audit trails, stores for 30days, longer needs to go in a s3 bucket and query through elastic search
Cloudtrail can trigger ~AWS CloudWatch~ alarms which then generates a ~eventbrdige/sns~ which can lead to an ~lambda~

***** Lake
CloudTrail Lake is a fully managed solution for capturing, storing, accessing, and analyzing user & API- related activity on AWS. It is designed to solve all problems that we have listed above with an out-of- the-box event History and S3/Athena patterns for storage and query.

Traditional solution: Create a CloudTrail trail & send the logs to an S3 bucket to store them securely. Use Athena to query the logs in S3 and store the results in another S3 bucket. This however takes time


***** Performant architecture
****** api activity to all region
Ensure one cloudtrail log is enabled for all regions

**** AWS Config
AWS Config provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time.

Great for auditing the changes to one's resource/app. Analogy librarian keeping track of the state and status of books throughout their lifetime

**** Artifacts
~AWS Artifact~ is a web service that enables you to download AWS security and compliance documents such as ISO certifications and SOC reports. Works for GDPR.

You can download the report and submit to the auditor directly

**** AWS GuardDuty
Amazon GuardDuty is a fully managed and advanced threat detection service providing broad protection to AWS Accounts and workloads. It helps to identify threats like attacker reconnaissance, instance compromise, account compromise, and bucket compromise.

Uses machine learning and threat inteligence. Collects logs such as ~cloudtrail~, ~vpc flow logs~, ~dns logs~ which then can trigger a ~lambda~.
If something happens, it generates a finding and gives it a severity score.
You can give it a trusted IP list (safe IPs) and a threat IP list (generated from org or taken elsewhere)

***** GuardDuty detection categories
- reconnainssance
- instance compromise
- account compromise
- bucket compromise

**** Inspector
Amazon Inspector is a security vulnerability assessment service that helps improve the security and compliance of your AWS resources. Amazon Inspector automatically assesses resources for vulnerabilities or deviations from best practices, and then produces a detailed list of security findings prioritized by level of severity.

You must specify the resources to scan (assessment target) _Focused on the workloads_
Check for the following:
- package vulnerability
- code vulnerability
- network reachability

**** AWS Macie
_Focused on s3_
Amazon Macie is a data security service that discovers sensitive data (Personnaly Identifiable Infromation ~PII~) by using machine learning and pattern matching, provides visibility into data security risks, and enables automated protection against those risks.

To help you manage the security posture of your organization's Amazon Simple Storage Service (Amazon S3) data estate, Macie provides you with an inventory of your S3 buckets, and automatically evaluates and monitors the buckets for security and access control.

**** Security hub
AWS Security Hub provides a consolidated view of your security status in AWS. Automate security checks, manage security findings, and identify the highest priority security issues across your AWS environment.

Central dashboard for the following:
- GuardDuty
- Inspector
- Macie
- External security tools
- lambda

Example flow:
~AWS Inspector~ detect a vulnerability in a ~AWS EC2~ Generate + share findings in ~AWS security Hub~ and triggers an event on ~AWS EventBridge~ or ~AWS SNS~ which invokes a ~lambda~ or ~step function~ or ~system manager~

**** KMS
AWS Key Management Service (AWS KMS) is an encryption and key management service scaled for the cloud. AWS KMS keys and functionality are used by other AWS services, and you can use them to protect data in your own applications that use AWS.

Can apply policies to keys dictating which users can use said key and what operation
keys can encrypt/decrypt/sign/verify files
kms monitoring: ~AWS cloudtrail~ ~AWS cloudwatch~
data key: encrypt large amount of data. Encrypted data key can be stored in the s3 bucket. We still need the kms key to decrypt the encrypted data key to paintext
imported key:
AWS managed keys: AWS manage them on behalf of us. little control used for services using encryption ~sqs~, ~s3~, ~ebs~

***** Asymmtric KMS keys
It is more secure as two keys are used here- one for encryption and the other for decryption.

Asymmetric KMS Keys represent a mathematically related RSA or elliptic curve (ECC) public and private key pair. The private key never leaves AWS KMS unencrypted. Asymmetric keys are used for digital signature applications such as trusted source code, authentication/authorization tokens, document e-signing, e-commerce transactions, and secure messaging.

Does not support encryption for services like s3, lambda, dynamoDB, etc.

***** Customer managed keys
 Customer managed keys are KMS keys in an AWS account that the customer creates, owns, and manages. The customer has full control over these KMS keys, including establishing and maintaining their key policies, IAM policies, grants, etc. Customer- managed keys do not support =digital signature verification=.

***** Symmetric keys (default)
Security is less as only one key is used for both encryption and decryption purpose.

S3, Lambda, DynamoDB, etc integrate with KMS, and only symmetric encryption KMS key can be used here to encrypt the data. Also, the requirement of having the same 256-bit encryption key to encrypt and decrypt the data indicates a Symmetric KMS Key.

**** CloudHSM
AWS CloudHSM combines the benefits of the AWS cloud with the security of hardware security modules (HSMs). A hardware security module (HSM) is a computing device that processes cryptographic operations and provides secure storage for cryptographic keys.

Central location to encrypt/decrypt data. keys never leave the physical server. AWS manages it for you in the cloud as opposed as you having on prem.

You have full control (not AWS) on the keys. Can use clusters for scalability

**** Certificate manager
AWS Certificate Manager (ACM) handles the complexity of creating, storing, and renewing public and private SSL/TLS X.509 certificates and keys that protect your AWS websites and applications. You can provide certificates for your integrated AWS services either by issuing them directly with ACM or by importing third-party certificates into the ACM management system

Careful you can blow the budget wiht them (form experience)

Can't be used for ~EC2/s3/lambda~. You should use for ~cloudfront~, ~ELB~, ~API Gateway~
region specific

***** Pending validation status
 The TLS certificate is expiring soon and needs to be renewed. Renew it by following the link in the email received by ACM regarding certificate expiration on any of the domain's WHOIS mailbox

**** Private Certificate authority
AWS Private CA enables creation of private certificate authority (CA) hierarchies, including root and subordinate CAs, without the investment and maintenance costs of operating an on-premises CA

Think of a country waiting to issue their bank notes
_Meant for internal communication only_ not on the internet

**** Secret Manager
AWS Secrets Manager helps you to securely encrypt, store, and retrieve credentials for your databases and other services. Instead of hardcoding credentials in your apps, you can make calls to Secrets Manager to retrieve your credentials whenever needed.
***** Automatic Secret Rotation:
Secrets Manager is ideal if you need automatic secret rotation. For example, rotating database credentials or API keys regularly without manual intervention.

***** Comprehensive Secret Management:
If you have complex secret management needs, like versioning, automatic rotation, or frequent updates to sensitive credentials, Secrets Manager provides a more advanced solution.

***** Detailed Monitoring and Compliance:
Secrets Manager offers detailed audit logging, which can help with compliance standards like GDPR, HIPAA, or SOC 2, and you need to tightly monitor who accesses your secrets.

**** NACL/Security group
Reminder NACL acts at the subnet level while Security group acts at the application level. SG are disable all by default. Each rule only add to the allow. SG rules are merged together e.g. if you have multiple SG for one EC2 they get merged into one.

***** Making changes while running
changes are applied immediately
***** Secure architecture
******
**** Security Lake
Amazon Security Lake is a fully managed security data lake service. You can use Security Lake to automatically centralize security data from AWS environments, SaaS providers, on premises, cloud sources, and third-party sources into a purpose-built data lake that's stored in your AWS account.

As any lake, it makes a copy of all the data.
collect -> store logs into s3 bucket -> normalilize AWS logs -> query/access data

**** WAF
AWS WAF is a web application firewall that lets you monitor the HTTP(S) requests that are forwarded to your protected web application resources. You can protect the following resource types:

sits behind ~cloudfront~, ~API gateway~, or ~ELB~
**** 1. AWS WAF Blanket Rate-based Rules (general)
Rate-based rules allow you to throttle requests based on the rate of traffic from specific IP addresses.

   - A blanket rate-based rule applies to all incoming requests to the ALB, limiting the number of requests from any IP address to mitigate against DDoS or brute force attacks.
   - AWS WAF counts requests for each originating IP address, and if the rate exceeds a defined threshold, the rule will block or allow traffic accordingly.
   - Useful for global rate-limiting across all endpoints behind the ALB.

   Example:
   - Attach this rule to throttle requests above a certain threshold for the entire ALB.
   - Ideal for protecting against excessive traffic from any IP address across all endpoints.

**** 2. AWS WAF URI-specific Rate-based Rules (resources)
This rule limits the rate of requests specifically targeting certain URIs or paths behind the ALB.

   - Similar to blanket rate-based rules, but URI-specific rules focus on requests to certain paths (e.g., `/login`, `/api/submit`).
   - Can be helpful to protect sensitive or high-value endpoints (e.g., login or checkout pages) from abuse or high traffic spikes.
   - Provides more granular protection by focusing only on specific parts of the application.

   Example:
   - Attach this rule to protect specific URIs like `/login` from brute force attacks, limiting the number of requests to that endpoint.

**** 3. AWS WAF IP Reputation Rate-based Rules (ip)
This rule type blocks or allows requests based on IP reputation.

   - Leverages AWS's internal threat intelligence or third-party feeds to assess the reputation of IP addresses.
   - Can automatically block traffic from known malicious IPs, protecting your application from botnets, spam, or IPs known for attacks.
   - Includes rate-based filtering if the IP reputation reaches a threshold of malicious behavior.

   Example:
   - Attach this rule to automatically block traffic from IPs with a bad reputation, enhancing security without manual intervention.

**** 4. AWS WAF Managed Rule Group Statements
Managed rule groups are pre-configured by AWS or third parties to provide a set of rules for common attack patterns.

   - These rule groups include protections against common web exploits like SQL injection, cross-site scripting (XSS), or specific known vulnerabilities.
   - Managed rule groups are updated automatically by AWS or the third-party provider, ensuring the latest protection without requiring manual maintenance.
   - AWS offers rule groups for general security, while others are more specialized (e.g., bot control, ATO protection).

   Example:
   - Attach a managed rule group to protect against OWASP Top 10 vulnerabilities without needing to configure custom rules.
   - Useful for quick, broad protection and ease of management.

**** shield
AWS provides two levels of protection against DDoS attacks: AWS Shield Standard and AWS Shield Advanced. AWS Shield Standard is automatically included at no extra cost beyond what you already pay for AWS WAF and your other AWS services. For _added protection against DDoS attacks_, AWS offers AWS Shield Advanced.

DDos consumes alot of your resources, AWS will not bill you for the resources taxed during the attack. You also get AWS shield response team (SRT)

**** Network Firewall
AWS Network Firewall is a stateful, managed, network firewall and intrusion detection and prevention service for your virtual private cloud (VPC) that you create in Amazon Virtual Private Cloud (Amazon VPC).

has to be in its own subnet and forward traffic to the subnet containing our resources. Must configure the ~route table~ proprely.

traffic comes from the ~IGW~ to the firewal subnet, goes to the firewal endpoint for inspection.

Can make use of a transit gateway so that you do not duplicate the firewall. otherwise its one per VPC.

**** Firewall Manager
AWS Firewall Manager simplifies your administration and maintenance tasks across multiple accounts and resources for a variety of protections, including AWS WAF, AWS Shield Advanced, Amazon VPC security groups and network ACLs, AWS Network Firewall, and Amazon Route 53 Resolver DNS Firewall

kinda like ~control tower~

** TODO Design [0/4]
DEADLINE: <2024-10-13 Sun>
*** TODO Security
*** TODO Reliability
*** TODO Performance
*** TODO Cost-optimization

** TODO Exams [/]
*** Old once a week
*** New 2 of them
DEADLINE: <2024-10-21 Mon>

* Misc
Ensures data is protect both in transit and at rest: use ~AWS KMS~
A healthcare company is planning to migrate its patient data management system to AWS using AWS Application Migration Service (AWS MGN). Given the sensitive nature of patient data, what should the Solutions Architect recommend to ensure data security during the migration process?

aws: SourceVpce vs aws: SourceVpc (one is endpoint/other is vpc)
